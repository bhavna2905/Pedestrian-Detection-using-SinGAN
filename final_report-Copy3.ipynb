{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SinGAN on 1 image of the Kitti Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LX5DLAoDiUJp",
    "outputId": "968e5ada-35e0-462c-d9ed-24a0a80bfafa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SinGAN' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tamarott/SinGAN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRwBbo3RiUVz",
    "outputId": "f37d0890-5dde-446c-9d4b-e5c1214788a4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: numpy in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from torch) (1.24.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 1)) (3.7.5)\n",
      "Requirement already satisfied: scikit-image in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 2)) (0.16.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: numpy in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 5)) (1.24.4)\n",
      "Requirement already satisfied: torch in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r SinGAN/requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r SinGAN/requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r SinGAN/requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r SinGAN/requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r SinGAN/requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r SinGAN/requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r SinGAN/requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from torch->-r SinGAN/requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: six in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->-r SinGAN/requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->-r SinGAN/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->-r SinGAN/requirements.txt (line 2)) (4.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "\n",
    "!pip install -r SinGAN/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNYqlIl1u7P2",
    "outputId": "0086fac2-c681-465e-be35-b04fbc8bcff5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhavnasharma/Downloads/SinGAN\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/bhavnasharma/Downloads/SinGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Exraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_file(label_file):\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            label = line.strip().split(' ')\n",
    "            labels.append({\n",
    "                \"type\": label[0],\n",
    "                \"left\": float(label[4]),\n",
    "                \"top\": float(label[5]),\n",
    "                \"right\": float(label[6]), \n",
    "                \"bottom\": float(label[7])\n",
    "            })\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop object so it doesnt get distorted during SinGAN and fill space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 000000_crop.png and 000000_filled.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def crop_and_fill_image(image_path, label_path, output_dir):\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    filled_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(filled_image)\n",
    "\n",
    "    labels = get_label_from_file(label_path)\n",
    "\n",
    "    for label in labels:\n",
    "        left = int(label[\"left\"])\n",
    "        top = int(label[\"top\"])\n",
    "        right = int(label[\"right\"])\n",
    "        bottom = int(label[\"bottom\"])\n",
    "\n",
    "        cropped_image = original_image.crop((left, top, right, bottom))\n",
    "\n",
    "        draw.rectangle([left, top, right, bottom], fill=(255, 255, 255))\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    cropped_filename = f\"{base_filename}_crop.png\"\n",
    "    filled_filename = f\"{base_filename}_filled.png\"\n",
    "\n",
    "    cropped_image.save(os.path.join(output_dir, cropped_filename))\n",
    "\n",
    "    filled_image.save(os.path.join(output_dir, filled_filename))\n",
    "\n",
    "    print(f\"Finished: {cropped_filename} and {filled_filename}\")\n",
    "\n",
    "image_path = '/Users/bhavnasharma/Downloads/data_object_image_2/training/image_2/000000.png'\n",
    "label_path = '/Users/bhavnasharma/Downloads/training/label_2/000000.txt'\n",
    "output_dir = '/Users/bhavnasharma/Downloads/SinGAN/Input/Images'\n",
    "\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "crop_and_fill_image(image_path, label_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create augmentations of Object to give variety of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "import numpy as np\n",
    "\n",
    "# Load the cropped pedestrian\n",
    "cropped_pedestrian = load_img('/Users/bhavnasharma/Downloads/SinGAN/Input/Images/000000_crop.png')\n",
    "cropped_pedestrian_array = img_to_array(cropped_pedestrian)\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    channel_shift_range=50.0\n",
    ")\n",
    "\n",
    "# Create a directory to save augmented samples if it doesn't exist\n",
    "augmented_samples_dir = '/Users/bhavnasharma/Downloads/SinGAN/aug_images'\n",
    "os.makedirs(augmented_samples_dir, exist_ok=True)\n",
    "\n",
    "# Generate and save 50 augmented samples\n",
    "i = 0\n",
    "for batch in datagen.flow(np.expand_dims(cropped_pedestrian_array, 0), batch_size=1, save_to_dir=augmented_samples_dir, save_prefix='aug', save_format='png'):\n",
    "    i += 1\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SinGAN and generate 50 backrounds from the 1 input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwPixYtajyah",
    "outputId": "d3d88393-b164-4992-8b30-50ebec15cd5b"
   },
   "outputs": [],
   "source": [
    "!python main_train.py --input_name 000000_filled.png --not_cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code was used in our previous run and the result model was saved for repeated use. \n",
    "# You can find the saved model in the SinGAN models folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXVpTIM8jyel",
    "outputId": "0394fb88-b8c8-4bca-fd6b-396e5016a647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  5358\r\n",
      "random samples for image 000000_filled.png, start scale=0, already exist\r\n"
     ]
    }
   ],
   "source": [
    "!python random_samples.py --input_name 000000_filled.png --mode random_samples --niter 5000 --gen_start_scale 0 --not_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paste the augmented object images over the different backrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Images 1/50, successful_pastes 3 targets\n",
      "Finished: Images 2/50, successful_pastes 3 targets\n",
      "Finished: Images 3/50, successful_pastes 1 targets\n",
      "Finished: Images 4/50, successful_pastes 5 targets\n",
      "Finished: Images 5/50, successful_pastes 3 targets\n",
      "Finished: Images 6/50, successful_pastes 4 targets\n",
      "Finished: Images 7/50, successful_pastes 5 targets\n",
      "Finished: Images 8/50, successful_pastes 0 targets\n",
      "Finished: Images 9/50, successful_pastes 1 targets\n",
      "Finished: Images 10/50, successful_pastes 1 targets\n",
      "Finished: Images 11/50, successful_pastes 2 targets\n",
      "Finished: Images 12/50, successful_pastes 5 targets\n",
      "Finished: Images 13/50, successful_pastes 2 targets\n",
      "Finished: Images 14/50, successful_pastes 0 targets\n",
      "Finished: Images 15/50, successful_pastes 2 targets\n",
      "Finished: Images 16/50, successful_pastes 3 targets\n",
      "Finished: Images 17/50, successful_pastes 4 targets\n",
      "Finished: Images 18/50, successful_pastes 3 targets\n",
      "Finished: Images 19/50, successful_pastes 2 targets\n",
      "Finished: Images 20/50, successful_pastes 4 targets\n",
      "Finished: Images 21/50, successful_pastes 2 targets\n",
      "Finished: Images 22/50, successful_pastes 3 targets\n",
      "Finished: Images 23/50, successful_pastes 3 targets\n",
      "Finished: Images 24/50, successful_pastes 3 targets\n",
      "Finished: Images 25/50, successful_pastes 3 targets\n",
      "Finished: Images 26/50, successful_pastes 2 targets\n",
      "Finished: Images 27/50, successful_pastes 3 targets\n",
      "Finished: Images 28/50, successful_pastes 2 targets\n",
      "Finished: Images 29/50, successful_pastes 3 targets\n",
      "Finished: Images 30/50, successful_pastes 1 targets\n",
      "Finished: Images 31/50, successful_pastes 1 targets\n",
      "Finished: Images 32/50, successful_pastes 2 targets\n",
      "Finished: Images 33/50, successful_pastes 3 targets\n",
      "Finished: Images 34/50, successful_pastes 3 targets\n",
      "Finished: Images 35/50, successful_pastes 1 targets\n",
      "Finished: Images 36/50, successful_pastes 3 targets\n",
      "Finished: Images 37/50, successful_pastes 1 targets\n",
      "Finished: Images 38/50, successful_pastes 2 targets\n",
      "Finished: Images 39/50, successful_pastes 1 targets\n",
      "Finished: Images 40/50, successful_pastes 3 targets\n",
      "Finished: Images 41/50, successful_pastes 1 targets\n",
      "Finished: Images 42/50, successful_pastes 5 targets\n",
      "Finished: Images 43/50, successful_pastes 2 targets\n",
      "Finished: Images 44/50, successful_pastes 2 targets\n",
      "Finished: Images 45/50, successful_pastes 2 targets\n",
      "Finished: Images 46/50, successful_pastes 2 targets\n",
      "Finished: Images 47/50, successful_pastes 1 targets\n",
      "Finished: Images 48/50, successful_pastes 1 targets\n",
      "Finished: Images 49/50, successful_pastes 4 targets\n",
      "Finished: Images 50/50, successful_pastes 3 targets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def resize_augmented_image(aug_img, bg_img, max_scale=0.3): \n",
    "    bg_width, bg_height = bg_img.size\n",
    "    aug_width, aug_height = aug_img.size\n",
    "\n",
    "    # random shrink image\n",
    "    scale = random.uniform(0.05, max_scale)\n",
    "\n",
    "    new_width = int(bg_width * scale)\n",
    "    new_width = max(1, new_width)\n",
    "    new_height = int(aug_height * new_width / aug_width)\n",
    "    new_height = max(1, new_height)\n",
    "\n",
    "    return aug_img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "def check_overlap(new_box, existing_boxes, margin=10):\n",
    "    x1, y1, x2, y2 = new_box\n",
    "    for box in existing_boxes:\n",
    "        ex1, ey1, ex2, ey2 = box\n",
    "        # add margin check\n",
    "        if not (x2 + margin < ex1 or x1 - margin > ex2 or\n",
    "                y2 + margin < ey1 or y1 - margin > ey2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_non_overlapping_position(bg_size, aug_size, existing_boxes, max_attempts=50):\n",
    "    \"\"\"find non overlap positions\"\"\"\n",
    "    bg_width, bg_height = bg_size\n",
    "    aug_width, aug_height = aug_size\n",
    "\n",
    "    if aug_width >= bg_width or aug_height >= bg_height:\n",
    "        return None\n",
    "\n",
    "    # effective random region (not outside background)\n",
    "    max_x = max(0, bg_width - aug_width)\n",
    "    max_y = max(0, bg_height - aug_height)\n",
    "\n",
    "    if max_x == 0 or max_y == 0:\n",
    "        return None\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        x = random.randint(0, max_x)\n",
    "        y = random.randint(0, max_y)\n",
    "        new_box = (x, y, x + aug_width, y + aug_height)\n",
    "\n",
    "        if not check_overlap(new_box, existing_boxes):\n",
    "            return x, y\n",
    "\n",
    "    return None  \n",
    "\n",
    "def create_label(image_width, image_height, obj_coords, obj_size):\n",
    "    \"\"\"Create YOLO format label\"\"\"\n",
    "    x, y = obj_coords\n",
    "    width, height = obj_size\n",
    "\n",
    "    x_center = (x + width/2) / image_width\n",
    "    y_center = (y + height/2) / image_height\n",
    "    norm_width = width / image_width\n",
    "    norm_height = height / image_height\n",
    "\n",
    "    x_center = max(0, min(1, x_center))\n",
    "    y_center = max(0, min(1, y_center))\n",
    "    norm_width = max(0, min(1, norm_width))\n",
    "    norm_height = max(0, min(1, norm_height))\n",
    "\n",
    "    return f\"0 {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\"\n",
    "\n",
    "def paste_images_and_create_labels(bg_dir, aug_dir, output_dir, label_dir):\n",
    "    \"\"\"Paste image to background\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "    bg_images = [f for f in os.listdir(bg_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    aug_images = [f for f in os.listdir(aug_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for i, bg_image_name in enumerate(bg_images):\n",
    "        bg_image = Image.open(os.path.join(bg_dir, bg_image_name))\n",
    "        bg_width, bg_height = bg_image.size\n",
    "\n",
    "        num_images = random.randint(1, 5)\n",
    "        existing_boxes = []\n",
    "        labels = []\n",
    "\n",
    "        successful_pastes = 0\n",
    "        max_attempts = num_images * 2 \n",
    "        attempts = 0\n",
    "\n",
    "        while successful_pastes < num_images and attempts < max_attempts:\n",
    "            aug_image_name = random.choice(aug_images)\n",
    "            aug_image = Image.open(os.path.join(aug_dir, aug_image_name))\n",
    "            aug_image = resize_augmented_image(aug_image, bg_image)\n",
    "            aug_width, aug_height = aug_image.size\n",
    "\n",
    "            # find non overlap position\n",
    "            position = find_non_overlapping_position(\n",
    "                (bg_width, bg_height),\n",
    "                (aug_width, aug_height),\n",
    "                existing_boxes\n",
    "            )\n",
    "\n",
    "            if position is not None:\n",
    "                x, y = position\n",
    "                bg_image.paste(aug_image, (x, y), aug_image if aug_image.mode == 'RGBA' else None)\n",
    "\n",
    "                # label area\n",
    "                existing_boxes.append((x, y, x + aug_width, y + aug_height))\n",
    "\n",
    "                # create label\n",
    "                label = create_label(bg_width, bg_height, (x, y), (aug_width, aug_height))\n",
    "                labels.append(label)\n",
    "\n",
    "                successful_pastes += 1\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        # Save\n",
    "        bg_image.save(os.path.join(output_dir, f\"{i:06d}.png\"))\n",
    "        with open(os.path.join(label_dir, f\"{i:06d}.txt\"), 'w') as f:\n",
    "            for label in labels:\n",
    "                f.write(label + '\\n')\n",
    "\n",
    "        print(f\"Finished: Images {i+1}/{len(bg_images)}, successful_pastes {successful_pastes} targets\")\n",
    "\n",
    "# set path\n",
    "bg_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Output/RandomSamples/000000_filled/gen_start_scale=0\"\n",
    "aug_dir = \"/Users/bhavnasharma/Downloads/SinGAN/aug_images\"\n",
    "output_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/images\"\n",
    "label_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/labels\"\n",
    "\n",
    "# process\n",
    "paste_images_and_create_labels(bg_dir, aug_dir, output_dir, label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SinGAN to increase the resolution of the produced images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Random Seed:  9301\n",
      "*** Train SinGAN for SR ***\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 0:[0/2000]\n",
      "scale 0:[25/2000]\n",
      "scale 0:[50/2000]\n",
      "scale 0:[75/2000]\n",
      "scale 0:[100/2000]\n",
      "scale 0:[125/2000]\n",
      "scale 0:[150/2000]\n",
      "scale 0:[175/2000]\n",
      "scale 0:[200/2000]\n",
      "scale 0:[225/2000]\n",
      "scale 0:[250/2000]\n",
      "scale 0:[275/2000]\n",
      "scale 0:[300/2000]\n",
      "scale 0:[325/2000]\n",
      "scale 0:[350/2000]\n",
      "scale 0:[375/2000]\n",
      "scale 0:[400/2000]\n",
      "scale 0:[425/2000]\n",
      "scale 0:[450/2000]\n",
      "scale 0:[475/2000]\n",
      "scale 0:[500/2000]\n",
      "scale 0:[525/2000]\n",
      "scale 0:[550/2000]\n",
      "scale 0:[575/2000]\n",
      "scale 0:[600/2000]\n",
      "scale 0:[625/2000]\n",
      "scale 0:[650/2000]\n",
      "scale 0:[675/2000]\n",
      "scale 0:[700/2000]\n",
      "scale 0:[725/2000]\n",
      "scale 0:[750/2000]\n",
      "scale 0:[775/2000]\n",
      "scale 0:[800/2000]\n",
      "scale 0:[825/2000]\n",
      "scale 0:[850/2000]\n",
      "scale 0:[875/2000]\n",
      "scale 0:[900/2000]\n",
      "scale 0:[925/2000]\n",
      "scale 0:[950/2000]\n",
      "scale 0:[975/2000]\n",
      "scale 0:[1000/2000]\n",
      "scale 0:[1025/2000]\n",
      "scale 0:[1050/2000]\n",
      "scale 0:[1075/2000]\n",
      "scale 0:[1100/2000]\n",
      "scale 0:[1125/2000]\n",
      "scale 0:[1150/2000]\n",
      "scale 0:[1175/2000]\n",
      "scale 0:[1200/2000]\n",
      "scale 0:[1225/2000]\n",
      "scale 0:[1250/2000]\n",
      "scale 0:[1275/2000]\n",
      "scale 0:[1300/2000]\n",
      "scale 0:[1325/2000]\n",
      "scale 0:[1350/2000]\n",
      "scale 0:[1375/2000]\n",
      "scale 0:[1400/2000]\n",
      "scale 0:[1425/2000]\n",
      "scale 0:[1450/2000]\n",
      "scale 0:[1475/2000]\n",
      "scale 0:[1500/2000]\n",
      "scale 0:[1525/2000]\n",
      "scale 0:[1550/2000]\n",
      "scale 0:[1575/2000]\n",
      "scale 0:[1600/2000]\n",
      "scale 0:[1625/2000]\n",
      "scale 0:[1650/2000]\n",
      "scale 0:[1675/2000]\n",
      "scale 0:[1700/2000]\n",
      "scale 0:[1725/2000]\n",
      "scale 0:[1750/2000]\n",
      "scale 0:[1775/2000]\n",
      "scale 0:[1800/2000]\n",
      "scale 0:[1825/2000]\n",
      "scale 0:[1850/2000]\n",
      "scale 0:[1875/2000]\n",
      "scale 0:[1900/2000]\n",
      "scale 0:[1925/2000]\n",
      "scale 0:[1950/2000]\n",
      "scale 0:[1975/2000]\n",
      "scale 0:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 1:[0/2000]\n",
      "scale 1:[25/2000]\n",
      "scale 1:[50/2000]\n",
      "scale 1:[75/2000]\n",
      "scale 1:[100/2000]\n",
      "scale 1:[125/2000]\n",
      "scale 1:[150/2000]\n",
      "scale 1:[175/2000]\n",
      "scale 1:[200/2000]\n",
      "scale 1:[225/2000]\n",
      "scale 1:[250/2000]\n",
      "scale 1:[275/2000]\n",
      "scale 1:[300/2000]\n",
      "scale 1:[325/2000]\n",
      "scale 1:[350/2000]\n",
      "scale 1:[375/2000]\n",
      "scale 1:[400/2000]\n",
      "scale 1:[425/2000]\n",
      "scale 1:[450/2000]\n",
      "scale 1:[475/2000]\n",
      "scale 1:[500/2000]\n",
      "scale 1:[525/2000]\n",
      "scale 1:[550/2000]\n",
      "scale 1:[575/2000]\n",
      "scale 1:[600/2000]\n",
      "scale 1:[625/2000]\n",
      "scale 1:[650/2000]\n",
      "scale 1:[675/2000]\n",
      "scale 1:[700/2000]\n",
      "scale 1:[725/2000]\n",
      "scale 1:[750/2000]\n",
      "scale 1:[775/2000]\n",
      "scale 1:[800/2000]\n",
      "scale 1:[825/2000]\n",
      "scale 1:[850/2000]\n",
      "scale 1:[875/2000]\n",
      "scale 1:[900/2000]\n",
      "scale 1:[925/2000]\n",
      "scale 1:[950/2000]\n",
      "scale 1:[975/2000]\n",
      "scale 1:[1000/2000]\n",
      "scale 1:[1025/2000]\n",
      "scale 1:[1050/2000]\n",
      "scale 1:[1075/2000]\n",
      "scale 1:[1100/2000]\n",
      "scale 1:[1125/2000]\n",
      "scale 1:[1150/2000]\n",
      "scale 1:[1175/2000]\n",
      "scale 1:[1200/2000]\n",
      "scale 1:[1225/2000]\n",
      "scale 1:[1250/2000]\n",
      "scale 1:[1275/2000]\n",
      "scale 1:[1300/2000]\n",
      "scale 1:[1325/2000]\n",
      "scale 1:[1350/2000]\n",
      "scale 1:[1375/2000]\n",
      "scale 1:[1400/2000]\n",
      "scale 1:[1425/2000]\n",
      "scale 1:[1450/2000]\n",
      "scale 1:[1475/2000]\n",
      "scale 1:[1500/2000]\n",
      "scale 1:[1525/2000]\n",
      "scale 1:[1550/2000]\n",
      "scale 1:[1575/2000]\n",
      "scale 1:[1600/2000]\n",
      "scale 1:[1625/2000]\n",
      "scale 1:[1650/2000]\n",
      "scale 1:[1675/2000]\n",
      "scale 1:[1700/2000]\n",
      "scale 1:[1725/2000]\n",
      "scale 1:[1750/2000]\n",
      "scale 1:[1775/2000]\n",
      "scale 1:[1800/2000]\n",
      "scale 1:[1825/2000]\n",
      "scale 1:[1850/2000]\n",
      "scale 1:[1875/2000]\n",
      "scale 1:[1900/2000]\n",
      "scale 1:[1925/2000]\n",
      "scale 1:[1950/2000]\n",
      "scale 1:[1975/2000]\n",
      "scale 1:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 2:[0/2000]\n",
      "scale 2:[25/2000]\n",
      "scale 2:[50/2000]\n",
      "scale 2:[75/2000]\n",
      "scale 2:[100/2000]\n",
      "scale 2:[125/2000]\n",
      "scale 2:[150/2000]\n",
      "scale 2:[175/2000]\n",
      "scale 2:[200/2000]\n",
      "scale 2:[225/2000]\n",
      "scale 2:[250/2000]\n",
      "scale 2:[275/2000]\n",
      "scale 2:[300/2000]\n",
      "scale 2:[325/2000]\n",
      "scale 2:[350/2000]\n",
      "scale 2:[375/2000]\n",
      "scale 2:[400/2000]\n",
      "scale 2:[425/2000]\n",
      "scale 2:[450/2000]\n",
      "scale 2:[475/2000]\n",
      "scale 2:[500/2000]\n",
      "scale 2:[525/2000]\n",
      "scale 2:[550/2000]\n",
      "scale 2:[575/2000]\n",
      "scale 2:[600/2000]\n",
      "scale 2:[625/2000]\n",
      "scale 2:[650/2000]\n",
      "scale 2:[675/2000]\n",
      "scale 2:[700/2000]\n",
      "scale 2:[725/2000]\n",
      "scale 2:[750/2000]\n",
      "scale 2:[775/2000]\n",
      "scale 2:[800/2000]\n",
      "scale 2:[825/2000]\n",
      "scale 2:[850/2000]\n",
      "scale 2:[875/2000]\n",
      "scale 2:[900/2000]\n",
      "scale 2:[925/2000]\n",
      "scale 2:[950/2000]\n",
      "scale 2:[975/2000]\n",
      "scale 2:[1000/2000]\n",
      "scale 2:[1025/2000]\n",
      "scale 2:[1050/2000]\n",
      "scale 2:[1075/2000]\n",
      "scale 2:[1100/2000]\n",
      "scale 2:[1125/2000]\n",
      "scale 2:[1150/2000]\n",
      "scale 2:[1175/2000]\n",
      "scale 2:[1200/2000]\n",
      "scale 2:[1225/2000]\n",
      "scale 2:[1250/2000]\n",
      "scale 2:[1275/2000]\n",
      "scale 2:[1300/2000]\n",
      "scale 2:[1325/2000]\n",
      "scale 2:[1350/2000]\n",
      "scale 2:[1375/2000]\n",
      "scale 2:[1400/2000]\n",
      "scale 2:[1425/2000]\n",
      "scale 2:[1450/2000]\n",
      "scale 2:[1475/2000]\n",
      "scale 2:[1500/2000]\n",
      "scale 2:[1525/2000]\n",
      "scale 2:[1550/2000]\n",
      "scale 2:[1575/2000]\n",
      "scale 2:[1600/2000]\n",
      "scale 2:[1625/2000]\n",
      "scale 2:[1650/2000]\n",
      "scale 2:[1675/2000]\n",
      "scale 2:[1700/2000]\n",
      "scale 2:[1725/2000]\n",
      "scale 2:[1750/2000]\n",
      "scale 2:[1775/2000]\n",
      "scale 2:[1800/2000]\n",
      "scale 2:[1825/2000]\n",
      "scale 2:[1850/2000]\n",
      "scale 2:[1875/2000]\n",
      "scale 2:[1900/2000]\n",
      "scale 2:[1925/2000]\n",
      "scale 2:[1950/2000]\n",
      "scale 2:[1975/2000]\n",
      "scale 2:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 3:[0/2000]\n",
      "scale 3:[25/2000]\n",
      "scale 3:[50/2000]\n",
      "scale 3:[75/2000]\n",
      "scale 3:[100/2000]\n",
      "scale 3:[125/2000]\n",
      "scale 3:[150/2000]\n",
      "scale 3:[175/2000]\n",
      "scale 3:[200/2000]\n",
      "scale 3:[225/2000]\n",
      "scale 3:[250/2000]\n",
      "scale 3:[275/2000]\n",
      "scale 3:[300/2000]\n",
      "scale 3:[325/2000]\n",
      "scale 3:[350/2000]\n",
      "scale 3:[375/2000]\n",
      "scale 3:[400/2000]\n",
      "scale 3:[425/2000]\n",
      "scale 3:[450/2000]\n",
      "scale 3:[475/2000]\n",
      "scale 3:[500/2000]\n",
      "scale 3:[525/2000]\n",
      "scale 3:[550/2000]\n",
      "scale 3:[575/2000]\n",
      "scale 3:[600/2000]\n",
      "scale 3:[625/2000]\n",
      "scale 3:[650/2000]\n",
      "scale 3:[675/2000]\n",
      "scale 3:[700/2000]\n",
      "scale 3:[725/2000]\n",
      "scale 3:[750/2000]\n",
      "scale 3:[775/2000]\n",
      "scale 3:[800/2000]\n",
      "scale 3:[825/2000]\n",
      "scale 3:[850/2000]\n",
      "scale 3:[875/2000]\n",
      "scale 3:[900/2000]\n",
      "scale 3:[925/2000]\n",
      "scale 3:[950/2000]\n",
      "scale 3:[975/2000]\n",
      "scale 3:[1000/2000]\n",
      "scale 3:[1025/2000]\n",
      "scale 3:[1050/2000]\n",
      "scale 3:[1075/2000]\n",
      "scale 3:[1100/2000]\n",
      "scale 3:[1125/2000]\n",
      "scale 3:[1150/2000]\n",
      "scale 3:[1175/2000]\n",
      "scale 3:[1200/2000]\n",
      "scale 3:[1225/2000]\n",
      "scale 3:[1250/2000]\n",
      "scale 3:[1275/2000]\n",
      "scale 3:[1300/2000]\n",
      "scale 3:[1325/2000]\n",
      "scale 3:[1350/2000]\n",
      "scale 3:[1375/2000]\n",
      "scale 3:[1400/2000]\n",
      "scale 3:[1425/2000]\n",
      "scale 3:[1450/2000]\n",
      "scale 3:[1475/2000]\n",
      "scale 3:[1500/2000]\n",
      "scale 3:[1525/2000]\n",
      "scale 3:[1550/2000]\n",
      "scale 3:[1575/2000]\n",
      "scale 3:[1600/2000]\n",
      "scale 3:[1625/2000]\n",
      "scale 3:[1650/2000]\n",
      "scale 3:[1675/2000]\n",
      "scale 3:[1700/2000]\n",
      "scale 3:[1725/2000]\n",
      "scale 3:[1750/2000]\n",
      "scale 3:[1775/2000]\n",
      "scale 3:[1800/2000]\n",
      "scale 3:[1825/2000]\n",
      "scale 3:[1850/2000]\n",
      "scale 3:[1875/2000]\n",
      "scale 3:[1900/2000]\n",
      "scale 3:[1925/2000]\n",
      "scale 3:[1950/2000]\n",
      "scale 3:[1975/2000]\n",
      "scale 3:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 4:[0/2000]\n",
      "scale 4:[25/2000]\n",
      "scale 4:[50/2000]\n",
      "scale 4:[75/2000]\n",
      "scale 4:[100/2000]\n",
      "scale 4:[125/2000]\n",
      "scale 4:[150/2000]\n",
      "scale 4:[175/2000]\n",
      "scale 4:[200/2000]\n",
      "scale 4:[225/2000]\n",
      "scale 4:[250/2000]\n",
      "scale 4:[275/2000]\n",
      "scale 4:[300/2000]\n",
      "scale 4:[325/2000]\n",
      "scale 4:[350/2000]\n",
      "scale 4:[375/2000]\n",
      "scale 4:[400/2000]\n",
      "scale 4:[425/2000]\n",
      "scale 4:[450/2000]\n",
      "scale 4:[475/2000]\n",
      "scale 4:[500/2000]\n",
      "scale 4:[525/2000]\n",
      "scale 4:[550/2000]\n",
      "scale 4:[575/2000]\n",
      "scale 4:[600/2000]\n",
      "scale 4:[625/2000]\n",
      "scale 4:[650/2000]\n",
      "scale 4:[675/2000]\n",
      "scale 4:[700/2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 4:[725/2000]\n",
      "scale 4:[750/2000]\n",
      "scale 4:[775/2000]\n",
      "scale 4:[800/2000]\n",
      "scale 4:[825/2000]\n",
      "scale 4:[850/2000]\n",
      "scale 4:[875/2000]\n",
      "scale 4:[900/2000]\n",
      "scale 4:[925/2000]\n",
      "scale 4:[950/2000]\n",
      "scale 4:[975/2000]\n",
      "scale 4:[1000/2000]\n",
      "scale 4:[1025/2000]\n",
      "scale 4:[1050/2000]\n",
      "scale 4:[1075/2000]\n",
      "scale 4:[1100/2000]\n",
      "scale 4:[1125/2000]\n",
      "scale 4:[1150/2000]\n",
      "scale 4:[1175/2000]\n",
      "scale 4:[1200/2000]\n",
      "scale 4:[1225/2000]\n",
      "scale 4:[1250/2000]\n",
      "scale 4:[1275/2000]\n",
      "scale 4:[1300/2000]\n",
      "scale 4:[1325/2000]\n",
      "scale 4:[1350/2000]\n",
      "scale 4:[1375/2000]\n",
      "scale 4:[1400/2000]\n",
      "scale 4:[1425/2000]\n",
      "scale 4:[1450/2000]\n",
      "scale 4:[1475/2000]\n",
      "scale 4:[1500/2000]\n",
      "scale 4:[1525/2000]\n",
      "scale 4:[1550/2000]\n",
      "scale 4:[1575/2000]\n",
      "scale 4:[1600/2000]\n",
      "scale 4:[1625/2000]\n",
      "scale 4:[1650/2000]\n",
      "scale 4:[1675/2000]\n",
      "scale 4:[1700/2000]\n",
      "scale 4:[1725/2000]\n",
      "scale 4:[1750/2000]\n",
      "scale 4:[1775/2000]\n",
      "scale 4:[1800/2000]\n",
      "scale 4:[1825/2000]\n",
      "scale 4:[1850/2000]\n",
      "scale 4:[1875/2000]\n",
      "scale 4:[1900/2000]\n",
      "scale 4:[1925/2000]\n",
      "scale 4:[1950/2000]\n",
      "scale 4:[1975/2000]\n",
      "scale 4:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 5:[0/2000]\n",
      "scale 5:[25/2000]\n",
      "scale 5:[50/2000]\n",
      "scale 5:[75/2000]\n",
      "scale 5:[100/2000]\n",
      "scale 5:[125/2000]\n",
      "scale 5:[150/2000]\n",
      "scale 5:[175/2000]\n",
      "scale 5:[200/2000]\n",
      "scale 5:[225/2000]\n",
      "scale 5:[250/2000]\n",
      "scale 5:[275/2000]\n",
      "scale 5:[300/2000]\n",
      "scale 5:[325/2000]\n",
      "scale 5:[350/2000]\n",
      "scale 5:[375/2000]\n",
      "scale 5:[400/2000]\n",
      "scale 5:[425/2000]\n",
      "scale 5:[450/2000]\n",
      "scale 5:[475/2000]\n",
      "scale 5:[500/2000]\n",
      "scale 5:[525/2000]\n",
      "scale 5:[550/2000]\n",
      "scale 5:[575/2000]\n",
      "scale 5:[600/2000]\n",
      "scale 5:[625/2000]\n",
      "scale 5:[650/2000]\n",
      "scale 5:[675/2000]\n",
      "scale 5:[700/2000]\n",
      "scale 5:[725/2000]\n",
      "scale 5:[750/2000]\n",
      "scale 5:[775/2000]\n",
      "scale 5:[800/2000]\n",
      "scale 5:[825/2000]\n",
      "scale 5:[850/2000]\n",
      "scale 5:[875/2000]\n",
      "scale 5:[900/2000]\n",
      "scale 5:[925/2000]\n",
      "scale 5:[950/2000]\n",
      "scale 5:[975/2000]\n",
      "scale 5:[1000/2000]\n",
      "scale 5:[1025/2000]\n",
      "scale 5:[1050/2000]\n",
      "scale 5:[1075/2000]\n",
      "scale 5:[1100/2000]\n",
      "scale 5:[1125/2000]\n",
      "scale 5:[1150/2000]\n",
      "scale 5:[1175/2000]\n",
      "scale 5:[1200/2000]\n",
      "scale 5:[1225/2000]\n",
      "scale 5:[1250/2000]\n",
      "scale 5:[1275/2000]\n",
      "scale 5:[1300/2000]\n",
      "scale 5:[1325/2000]\n",
      "scale 5:[1350/2000]\n",
      "scale 5:[1375/2000]\n",
      "scale 5:[1400/2000]\n",
      "scale 5:[1425/2000]\n",
      "scale 5:[1450/2000]\n",
      "scale 5:[1475/2000]\n",
      "scale 5:[1500/2000]\n",
      "scale 5:[1525/2000]\n",
      "scale 5:[1550/2000]\n",
      "scale 5:[1575/2000]\n",
      "scale 5:[1600/2000]\n",
      "scale 5:[1625/2000]\n",
      "scale 5:[1650/2000]\n",
      "scale 5:[1675/2000]\n",
      "scale 5:[1700/2000]\n",
      "scale 5:[1725/2000]\n",
      "scale 5:[1750/2000]\n",
      "scale 5:[1775/2000]\n",
      "scale 5:[1800/2000]\n",
      "scale 5:[1825/2000]\n",
      "scale 5:[1850/2000]\n",
      "scale 5:[1875/2000]\n",
      "scale 5:[1900/2000]\n",
      "scale 5:[1925/2000]\n",
      "scale 5:[1950/2000]\n",
      "scale 5:[1975/2000]\n",
      "scale 5:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 6:[0/2000]\n",
      "scale 6:[25/2000]\n",
      "scale 6:[50/2000]\n",
      "scale 6:[75/2000]\n",
      "scale 6:[100/2000]\n",
      "scale 6:[125/2000]\n",
      "scale 6:[150/2000]\n",
      "scale 6:[175/2000]\n",
      "scale 6:[200/2000]\n",
      "scale 6:[225/2000]\n",
      "scale 6:[250/2000]\n",
      "scale 6:[275/2000]\n",
      "scale 6:[300/2000]\n",
      "scale 6:[325/2000]\n",
      "scale 6:[350/2000]\n",
      "scale 6:[375/2000]\n",
      "scale 6:[400/2000]\n",
      "scale 6:[425/2000]\n",
      "scale 6:[450/2000]\n",
      "scale 6:[475/2000]\n",
      "scale 6:[500/2000]\n",
      "scale 6:[525/2000]\n",
      "scale 6:[550/2000]\n",
      "scale 6:[575/2000]\n",
      "scale 6:[600/2000]\n",
      "scale 6:[625/2000]\n",
      "scale 6:[650/2000]\n",
      "scale 6:[675/2000]\n",
      "scale 6:[700/2000]\n",
      "scale 6:[725/2000]\n",
      "scale 6:[750/2000]\n",
      "scale 6:[775/2000]\n",
      "scale 6:[800/2000]\n",
      "scale 6:[825/2000]\n",
      "scale 6:[850/2000]\n",
      "scale 6:[875/2000]\n",
      "scale 6:[900/2000]\n",
      "scale 6:[925/2000]\n",
      "scale 6:[950/2000]\n",
      "scale 6:[975/2000]\n",
      "scale 6:[1000/2000]\n",
      "scale 6:[1025/2000]\n",
      "scale 6:[1050/2000]\n",
      "scale 6:[1075/2000]\n",
      "scale 6:[1100/2000]\n",
      "scale 6:[1125/2000]\n",
      "scale 6:[1150/2000]\n",
      "scale 6:[1175/2000]\n",
      "scale 6:[1200/2000]\n",
      "scale 6:[1225/2000]\n",
      "scale 6:[1250/2000]\n",
      "scale 6:[1275/2000]\n",
      "scale 6:[1300/2000]\n",
      "scale 6:[1325/2000]\n",
      "scale 6:[1350/2000]\n",
      "scale 6:[1375/2000]\n",
      "scale 6:[1400/2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 6:[1425/2000]\n",
      "scale 6:[1450/2000]\n",
      "scale 6:[1475/2000]\n",
      "scale 6:[1500/2000]\n",
      "scale 6:[1525/2000]\n",
      "scale 6:[1550/2000]\n",
      "scale 6:[1575/2000]\n",
      "scale 6:[1600/2000]\n",
      "scale 6:[1625/2000]\n",
      "scale 6:[1650/2000]\n",
      "scale 6:[1675/2000]\n",
      "scale 6:[1700/2000]\n",
      "scale 6:[1725/2000]\n",
      "scale 6:[1750/2000]\n",
      "scale 6:[1775/2000]\n",
      "scale 6:[1800/2000]\n",
      "scale 6:[1825/2000]\n",
      "scale 6:[1850/2000]\n",
      "scale 6:[1875/2000]\n",
      "scale 6:[1900/2000]\n",
      "scale 6:[1925/2000]\n",
      "scale 6:[1950/2000]\n",
      "scale 6:[1975/2000]\n",
      "scale 6:[1999/2000]\n",
      "GeneratorConcatSkip2CleanAdd(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Sequential(\n",
      "    (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "WDiscriminator(\n",
      "  (head): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (block1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (block3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (LeakyRelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (tail): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n",
      "scale 7:[0/2000]\n",
      "scale 7:[25/2000]\n",
      "scale 7:[50/2000]\n",
      "scale 7:[75/2000]\n",
      "scale 7:[100/2000]\n",
      "scale 7:[125/2000]\n",
      "scale 7:[150/2000]\n",
      "scale 7:[175/2000]\n",
      "scale 7:[200/2000]\n",
      "scale 7:[225/2000]\n",
      "scale 7:[250/2000]\n",
      "scale 7:[275/2000]\n",
      "scale 7:[300/2000]\n",
      "scale 7:[325/2000]\n",
      "scale 7:[350/2000]\n",
      "scale 7:[375/2000]\n",
      "scale 7:[400/2000]\n",
      "scale 7:[425/2000]\n",
      "scale 7:[450/2000]\n",
      "scale 7:[475/2000]\n",
      "scale 7:[500/2000]\n",
      "scale 7:[525/2000]\n",
      "scale 7:[550/2000]\n",
      "scale 7:[575/2000]\n",
      "scale 7:[600/2000]\n",
      "scale 7:[625/2000]\n",
      "scale 7:[650/2000]\n",
      "scale 7:[675/2000]\n",
      "scale 7:[700/2000]\n",
      "scale 7:[725/2000]\n",
      "scale 7:[750/2000]\n",
      "scale 7:[775/2000]\n",
      "scale 7:[800/2000]\n",
      "scale 7:[825/2000]\n",
      "scale 7:[850/2000]\n",
      "scale 7:[875/2000]\n",
      "scale 7:[900/2000]\n",
      "scale 7:[925/2000]\n",
      "scale 7:[950/2000]\n",
      "scale 7:[975/2000]\n",
      "scale 7:[1000/2000]\n",
      "scale 7:[1025/2000]\n",
      "scale 7:[1050/2000]\n",
      "scale 7:[1075/2000]\n",
      "scale 7:[1100/2000]\n",
      "scale 7:[1125/2000]\n",
      "scale 7:[1150/2000]\n",
      "scale 7:[1175/2000]\n",
      "scale 7:[1200/2000]\n",
      "scale 7:[1225/2000]\n",
      "scale 7:[1250/2000]\n",
      "scale 7:[1275/2000]\n",
      "scale 7:[1300/2000]\n",
      "scale 7:[1325/2000]\n",
      "scale 7:[1350/2000]\n",
      "scale 7:[1375/2000]\n",
      "scale 7:[1400/2000]\n",
      "scale 7:[1425/2000]\n",
      "scale 7:[1450/2000]\n",
      "scale 7:[1475/2000]\n",
      "scale 7:[1500/2000]\n",
      "scale 7:[1525/2000]\n",
      "scale 7:[1550/2000]\n",
      "scale 7:[1575/2000]\n",
      "scale 7:[1600/2000]\n",
      "scale 7:[1625/2000]\n",
      "scale 7:[1650/2000]\n",
      "scale 7:[1675/2000]\n",
      "scale 7:[1700/2000]\n",
      "scale 7:[1725/2000]\n",
      "scale 7:[1750/2000]\n",
      "scale 7:[1775/2000]\n",
      "scale 7:[1800/2000]\n",
      "scale 7:[1825/2000]\n",
      "scale 7:[1850/2000]\n",
      "scale 7:[1875/2000]\n",
      "scale 7:[1900/2000]\n",
      "scale 7:[1925/2000]\n",
      "scale 7:[1950/2000]\n",
      "scale 7:[1975/2000]\n",
      "scale 7:[1999/2000]\n",
      "4.000000\n"
     ]
    }
   ],
   "source": [
    "!python SR.py --input_name 0.png --not_cuda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above process takes 1 image and trains on it till optimised resolution is reached. Due to the resources required for each image to be processed we have continued with the original resolution instead of increasing it. However, theoretically this process should increase the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection using YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
      "Requirement already satisfied: matplotlib in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (3.7.5)\n",
      "Requirement already satisfied: scikit-image in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.16.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: numpy in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.24.4)\n",
      "Requirement already satisfied: torch in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from torch->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: six in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/bhavnasharma/opt/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 2)) (4.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git \n",
    "!cd yolov5\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train-Test with a 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 40 images, Test set: 10 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(images_dir, labels_dir, train_images_dir, train_labels_dir, test_images_dir, test_labels_dir, train_ratio=0.8):\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(train_labels_dir, exist_ok=True)\n",
    "    os.makedirs(test_images_dir, exist_ok=True)\n",
    "    os.makedirs(test_labels_dir, exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.png')]\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Split the data\n",
    "    train_size = int(len(image_files) * train_ratio)\n",
    "    train_files = image_files[:train_size]\n",
    "    test_files = image_files[train_size:]\n",
    "\n",
    "    # Move the files \n",
    "    for file in train_files:\n",
    "        shutil.move(os.path.join(images_dir, file), os.path.join(train_images_dir, file))\n",
    "        shutil.move(os.path.join(labels_dir, file.replace('.png', '.txt')), os.path.join(train_labels_dir, file.replace('.png', '.txt')))\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.move(os.path.join(images_dir, file), os.path.join(test_images_dir, file))\n",
    "        shutil.move(os.path.join(labels_dir, file.replace('.png', '.txt')), os.path.join(test_labels_dir, file.replace('.png', '.txt')))\n",
    "\n",
    "    print(f\"Train set: {len(train_files)} images, Test set: {len(test_files)} images\")\n",
    "\n",
    "# Define directories\n",
    "images_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/images\"\n",
    "labels_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/labels\"\n",
    "train_images_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/train/images\"\n",
    "train_labels_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/train/labels\"\n",
    "test_images_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/test/images\"\n",
    "test_labels_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results/test/labels\"\n",
    "\n",
    "split_dataset(images_dir, labels_dir, train_images_dir, train_labels_dir, test_images_dir, test_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved successfully!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source and destination\n",
    "train_images_src = \"/Users/bhavnasharma/Downloads/SinGAN/Results/train/images\"\n",
    "train_labels_src = \"/Users/bhavnasharma/Downloads/SinGAN/Results/train/labels\"\n",
    "test_images_src = \"/Users/bhavnasharma/Downloads/SinGAN/Results/test/images\"\n",
    "test_labels_src = \"/Users/bhavnasharma/Downloads/SinGAN/Results/test/labels\"\n",
    "\n",
    "train_images_dst = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/images/train\"\n",
    "train_labels_dst = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train\"\n",
    "val_images_dst = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/images/val\"\n",
    "val_labels_dst = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val\"\n",
    "\n",
    "os.makedirs(train_images_dst, exist_ok=True)\n",
    "os.makedirs(train_labels_dst, exist_ok=True)\n",
    "os.makedirs(val_images_dst, exist_ok=True)\n",
    "os.makedirs(val_labels_dst, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(train_images_src):\n",
    "    shutil.move(os.path.join(train_images_src, file_name), os.path.join(train_images_dst, file_name))\n",
    "\n",
    "for file_name in os.listdir(train_labels_src):\n",
    "    shutil.move(os.path.join(train_labels_src, file_name), os.path.join(train_labels_dst, file_name))\n",
    "\n",
    "for file_name in os.listdir(test_images_src):\n",
    "    shutil.move(os.path.join(test_images_src, file_name), os.path.join(val_images_dst, file_name))\n",
    "\n",
    "for file_name in os.listdir(test_labels_src):\n",
    "    shutil.move(os.path.join(test_labels_src, file_name), os.path.join(val_labels_dst, file_name))\n",
    "\n",
    "print(\"Files moved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhavnasharma/Downloads/SinGAN/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/bhavnasharma/Downloads/SinGAN/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test YOLO performance on entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train...\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val...:   \u001b[0mError decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val... 18 \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.68 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp3/labels.jpg... \n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1246    0.04501          0         84        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45    0.00407      0.489    0.00493   0.000981\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1198    0.04742          0        110        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45    0.00925        0.4      0.008    0.00151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1139    0.04754          0         71        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45    0.00886        0.6     0.0164    0.00332\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1062    0.05443          0         87        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.011      0.578     0.0266    0.00546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.0996    0.05582          0         92        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45     0.0288      0.444     0.0451     0.0124\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G    0.09346    0.05572          0         82        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45     0.0782      0.347     0.0888     0.0243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G    0.08863    0.05553          0         76        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45     0.0998     0.0667     0.0923     0.0296\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G    0.08898    0.06373          0         94        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.217      0.156      0.158     0.0411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.08726    0.05794          0         83        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.241      0.578      0.214     0.0587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.08188    0.05702          0         91        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.316      0.617      0.277     0.0838\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.07923    0.05833          0        102        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.473      0.644      0.488      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.07635    0.05259          0         72        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.104      0.178     0.0682     0.0168\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.07492    0.05294          0         95        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45     0.0696      0.289     0.0538     0.0148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.07518    0.05423          0         99        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.532      0.607      0.643      0.163\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.06407    0.05126          0         77        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45     0.0738      0.578     0.0656     0.0212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.07619    0.04348          0         95        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.238      0.511       0.29     0.0861\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.06477    0.04581          0         68        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.521      0.507      0.515      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.06688    0.04187          0         91        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.457      0.542       0.46      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G     0.0653     0.0457          0         78        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.577      0.607      0.656      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.06255    0.04688          0        107        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.404      0.557      0.441       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.06506    0.04567          0         92        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.758      0.733      0.773      0.302\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G     0.0602    0.04436          0         97        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.574      0.733      0.655      0.239\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G     0.0604    0.03662          0         98        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.718      0.756      0.806      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.05723    0.03966          0         97        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.661      0.711      0.731      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.06127    0.03846          0         92        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.769      0.739      0.838      0.397\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.05471    0.03667          0         92        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.739        0.8      0.836       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.05296    0.03444          0         94        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.569      0.689      0.706      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.05901    0.03803          0         96        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45       0.57      0.824      0.648      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G     0.0523    0.03418          0         80        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45       0.57      0.824      0.648      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.05358    0.03714          0        107        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45       0.62      0.822      0.783      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      30/49         0G    0.05071    0.03478          0         80        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.605      0.815       0.73      0.354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G    0.05011    0.03652          0         88        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.653        0.8      0.815      0.447\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.05262    0.03465          0         79        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.653        0.8      0.815      0.447\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.04938    0.03133          0        105        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.767       0.88      0.854       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G     0.0502    0.03135          0         88        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45       0.45        0.8      0.557      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.05674    0.03211          0         97        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.599      0.864      0.693      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.04999    0.03187          0         87        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.599      0.864      0.693      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.04441    0.03428          0        103        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.703       0.84      0.845       0.56\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G     0.0434    0.03074          0         92        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.695      0.889      0.857       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.05117      0.031          0         88        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.825      0.867      0.916      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G    0.03933    0.03464          0        115        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.825      0.867      0.916      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.04035    0.03278          0        116        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.653      0.822      0.823      0.497\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.03861    0.03207          0        117        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.938      0.933      0.968       0.69\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.03717     0.0274          0         87        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.806      0.889      0.917      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.03883    0.02946          0        109        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.806      0.889      0.917      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G    0.03542    0.03276          0        108        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.915      0.954      0.952      0.705\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.03396    0.02711          0         90        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45       0.95      0.933      0.985      0.642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.03519    0.02786          0         94        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.998      0.978      0.991      0.702\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.03246    0.03152          0         91        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.998      0.978      0.991      0.702\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G      0.032    0.02959          0         94        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.978      0.974       0.99      0.736\n",
      "\n",
      "50 epochs completed in 1.071 hours.\n",
      "Optimizer stripped from runs/train/exp3/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp3/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp3/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.978      0.974       0.99      0.736\n",
      "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 50 --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml --weights yolov5s.pt --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.978      0.975       0.99      0.736\n",
      "Speed: 1.2ms pre-process, 141.7ms inference, 2.5ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/last.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         18         45      0.978      0.975       0.99      0.736\n",
      "Speed: 1.2ms pre-process, 168.4ms inference, 1.6ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/last.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset to show that increasing number of images is improving our performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Added to images/train_100.0\n",
      "12\n",
      "Added to images/train_250.0\n",
      "24\n",
      "Added to images/train_500.0\n",
      "36\n",
      "Added to images/train_750.0\n",
      "49\n",
      "Added to images/train_1000.0\n",
      "Dataset subsets created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Parameters\n",
    "split_ratios = [0.1, 0.25, 0.5, 0.75, 1.0]  # 10%, 25%, 50%, 75%, 100%\n",
    "images_dir = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/images/train\"\n",
    "labels_dir = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train\"\n",
    "base_dir = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data\"\n",
    "\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith('.png')]\n",
    "\n",
    "random.shuffle(image_files)\n",
    "\n",
    "for ratio in split_ratios:\n",
    "    size = int(len(image_files) * ratio)\n",
    "    print(size)\n",
    "    subset = image_files[:size]\n",
    "    train_images_dir = os.path.join(base_dir, f'images/train_{int(ratio*1000)}')\n",
    "    train_labels_dir = os.path.join(base_dir, f'labels/train_{int(ratio*1000)}')\n",
    "    print(\"Added to\", f'images/train_{ratio*1000}')\n",
    "    \n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(train_labels_dir, exist_ok=True)\n",
    "    \n",
    "    for file in subset:\n",
    "        shutil.copy(os.path.join(images_dir, file), os.path.join(train_images_dir, file))\n",
    "        shutil.copy(os.path.join(labels_dir, file.replace('.png', '.txt')), os.path.join(train_labels_dir, file.replace('.png', '.txt')))\n",
    "\n",
    "print(\"Dataset subsets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yaml files created for each subset!\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/Users/bhavnasharma/Downloads/SinGAN/yolov5/data'\n",
    "val_images_dir = \"/Users/bhavnasharma/Downloads/SinGAN/yolov5/data/images/val\"\n",
    "subsets = [100, 250, 500, 750, 1000]\n",
    "\n",
    "for size in subsets:\n",
    "    data_yaml_content = f\"\"\"\n",
    "    train: {base_dir}/images/train_{size}\n",
    "    val: {val_images_dir}\n",
    "\n",
    "    # Classes\n",
    "    nc: 1  # number of classes\n",
    "    names: ['Pedestrian']\n",
    "    \"\"\"\n",
    "    with open(f'data_{size}.yaml', 'w') as file:\n",
    "        file.write(data_yaml_content.strip())\n",
    "\n",
    "print(\"data.yaml files created for each subset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_100.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_10\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_100.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val... 24 \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.62 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp21/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp21\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1279    0.04897          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00258      0.304    0.00261   0.000655\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G      0.127    0.04944          0         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.393    0.00535   0.000857\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1235    0.04736          0         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00304      0.375    0.00447   0.000823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1236    0.04781          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.393      0.004   0.000772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1238    0.05352          0         30        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00425      0.339    0.00452   0.000862\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G     0.1247    0.05288          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.393    0.00386   0.000778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G     0.1258    0.04815          0         24        640: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.357    0.00362   0.000782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G      0.119    0.05246          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.393    0.00343   0.000729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G     0.1216    0.05143          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00361      0.464    0.00435   0.000903\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G     0.1248    0.03992          0         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00361      0.464    0.00458   0.000974\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G     0.1187    0.06035          0         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00433      0.464    0.00506    0.00107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G     0.1189    0.04735          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00379      0.446    0.00474    0.00113\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G      0.122    0.05511          0         30        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00391      0.482    0.00534    0.00127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G     0.1159     0.0732          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00389        0.5    0.00532    0.00143\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G     0.1218    0.04932          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00563      0.464    0.00603    0.00156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G     0.1165     0.0543          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00514      0.464    0.00754    0.00173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G     0.1113    0.04962          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0089      0.304    0.00897    0.00186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G     0.1152    0.03892          0         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00863      0.286    0.00864    0.00186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G     0.1137     0.0641          0         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00884      0.321    0.00993    0.00214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G     0.1135    0.05391          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00884      0.321    0.00993    0.00214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G     0.1098     0.0506          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00814      0.411     0.0108    0.00225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G     0.1079      0.056          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00814      0.411     0.0108    0.00225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G     0.1141    0.04992          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.008      0.571     0.0137    0.00268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G     0.1107    0.04693          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.008      0.571     0.0137    0.00268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G     0.1125    0.05077          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00873      0.518     0.0134    0.00278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G     0.1076    0.06052          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00873      0.518     0.0134    0.00278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G     0.1026     0.0578          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00937      0.607      0.016    0.00333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G     0.1097    0.06052          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00937      0.607      0.016    0.00333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G     0.1024    0.04712          0         18        640: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00922      0.482     0.0163    0.00363\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G     0.1038    0.06312          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00922      0.482     0.0163    0.00363\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G      0.104    0.04827          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0105      0.536     0.0215    0.00458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G      0.103    0.06925          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0105      0.536     0.0215    0.00458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G     0.1038    0.05894          0         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0133      0.411     0.0247     0.0048\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G      0.105    0.04224          0         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0133      0.411     0.0247     0.0048\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G     0.1005    0.05824          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.013      0.411     0.0221    0.00487\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G     0.1004      0.073          0         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.013      0.411     0.0221    0.00487\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.09977    0.03687          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0164      0.446     0.0227    0.00578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G     0.1083     0.0338          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0164      0.446     0.0227    0.00578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G     0.1062    0.06122          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0212      0.464     0.0229    0.00621\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G     0.1049    0.05277          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0212      0.464     0.0229    0.00621\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G      0.103    0.05916          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0232      0.446     0.0239    0.00606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G     0.1014    0.06899          0         30        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0232      0.446     0.0239    0.00606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G     0.1049    0.04822          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0269      0.446     0.0243    0.00627\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.09994    0.07267          0         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0269      0.446     0.0243    0.00627\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G     0.1005    0.07986          0         36        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0269      0.446     0.0258    0.00657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G      0.106    0.06724          0         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0269      0.446     0.0258    0.00657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G      0.103    0.04432          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0271      0.393     0.0277    0.00704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.09728    0.09098          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0271      0.393     0.0277    0.00704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.09929    0.07088          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0323      0.321     0.0295    0.00738\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.09365    0.05697          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0323      0.321     0.0295    0.00738\n",
      "\n",
      "50 epochs completed in 0.160 hours.\n",
      "Optimizer stripped from runs/train/exp21/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp21/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp21/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0347      0.321       0.03    0.00753\n",
      "Results saved to \u001b[1mruns/train/exp21\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_250.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_25\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_250.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.80 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp22/labels.jpg... \n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp22\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1238     0.0511          0         84        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00264      0.339    0.00251    0.00065\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1258    0.04445          0         67        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.339    0.00569   0.000933\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1244    0.04374          0         61        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.393    0.00425   0.000794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1222    0.04538          0         66        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.393    0.00425    0.00086\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1251    0.04766          0         77        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.375    0.00376   0.000828\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G     0.1219    0.05024          0         81        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.429    0.00341   0.000784\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G     0.1191    0.04864          0         71        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00361      0.464    0.00458    0.00104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G     0.1207    0.04829          0         74        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00409      0.482     0.0056    0.00123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G     0.1154    0.04962          0         72        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00422      0.536    0.00686    0.00159\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G     0.1132    0.05151          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00562      0.482    0.00888     0.0019\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G     0.1125    0.04778          0         63        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00742      0.571     0.0118     0.0024\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G     0.1105    0.05152          0         72        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00825      0.518     0.0144    0.00296\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G     0.1055    0.05039          0         60        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00906      0.554     0.0162    0.00359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G     0.1053    0.05533          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00949      0.518      0.017    0.00409\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G     0.1045    0.05538          0         69        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0362      0.125     0.0222    0.00471\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G     0.1001    0.07255          0         93        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0504      0.107      0.026    0.00573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.09757    0.05567          0         66        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0236      0.232     0.0208    0.00448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.09829    0.05303          0         65        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0236      0.232     0.0208    0.00448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.09942    0.04431          0         50        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0117      0.554     0.0222    0.00426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.09537     0.0641          0         82        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0117      0.554     0.0222    0.00426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.09735    0.05992          0         76        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0471      0.232     0.0425    0.00894\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.09201    0.06425          0         76        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0471      0.232     0.0425    0.00894\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.09647    0.07183          0         91        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0482       0.25      0.048     0.0122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.09122    0.06554          0         79        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0482       0.25      0.048     0.0122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.09217    0.05772          0         66        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0424      0.357      0.039    0.00942\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.09389    0.06121          0         77        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0424      0.357      0.039    0.00942\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.09288    0.05469          0         68        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0597      0.143     0.0567     0.0174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.08863    0.07151          0         84        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0597      0.143     0.0567     0.0174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.09385    0.05162          0         63        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.142      0.143     0.0945     0.0346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      29/49         0G    0.09141    0.07216          0         88        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.142      0.143     0.0945     0.0346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G    0.08944    0.06903          0         83        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.168     0.0893      0.106     0.0387\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G     0.0871    0.06452          0         76        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.168     0.0893      0.106     0.0387\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.08938    0.05788          0         67        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.205     0.0893      0.136     0.0357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.08773    0.06064          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.205     0.0893      0.136     0.0357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G     0.0852    0.06938          0         80        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.211      0.143      0.117     0.0322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.08718     0.0628          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.211      0.143      0.117     0.0322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.08991    0.07373          0         90        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.208      0.143      0.128     0.0334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.08789    0.06506          0         77        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.208      0.143      0.128     0.0334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G    0.08564    0.06105          0         74        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.139       0.25       0.12     0.0286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.08874    0.05666          0         69        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.139       0.25       0.12     0.0286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G    0.08281    0.06986          0         83        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.176      0.214      0.134     0.0348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.08525    0.06807          0         82        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.176      0.214      0.134     0.0348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.07885    0.05077          0         54        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.165      0.214      0.142     0.0351\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.08538    0.06938          0         84        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.165      0.214      0.142     0.0351\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.07999     0.0595          0         65        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.179      0.304      0.153     0.0425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G    0.08451    0.07153          0         86        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.179      0.304      0.153     0.0425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.08217    0.06636          0         78        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.199      0.321       0.17     0.0507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.08978    0.06712          0         86        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.199      0.321       0.17     0.0507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.08186    0.05611          0         63        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.235      0.339      0.186      0.054\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.08414    0.05455          0         67        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.235      0.339      0.186      0.054\n",
      "\n",
      "50 epochs completed in 0.389 hours.\n",
      "Optimizer stripped from runs/train/exp22/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp22/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp22/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.237      0.339      0.187     0.0536\n",
      "Results saved to \u001b[1mruns/train/exp22\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_500.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_50\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_500.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.71 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp23/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp23\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1248    0.04813          0         43        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00306      0.393    0.00517   0.000806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1246    0.04554          0         45        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.411    0.00334   0.000764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1241     0.0476          0         56        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.429    0.00398   0.000833\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1187    0.04686          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00361      0.464    0.00519    0.00125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1141    0.04749          0         43        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00658      0.518    0.00816    0.00189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G     0.1102      0.046          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00713      0.482      0.016    0.00375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G     0.1066    0.06123          0         67        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00919      0.589      0.022    0.00467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G     0.1013    0.06173          0         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00866      0.643     0.0214    0.00463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.09558    0.05501          0         46        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0138      0.589     0.0257    0.00541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.09407    0.05596          0         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0251      0.518     0.0344    0.00794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.09266     0.0601          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0173      0.625     0.0393    0.00957\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.09012    0.06977          0         55        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0538      0.482     0.0614     0.0148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.08621    0.06439          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0391      0.589     0.0759     0.0206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.08474    0.06338          0         50        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.126      0.482      0.119      0.032\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.08344    0.07336          0         56        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.167      0.357      0.147     0.0415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.08671    0.06102          0         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.201      0.319      0.174     0.0482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.07842    0.05551          0         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.278      0.429      0.232     0.0709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model for each dataset size\n",
    "!python train.py --img 640 --batch 16 --epochs 50 --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_100.yaml --weights yolov5s.pt --device cpu\n",
    "!python train.py --img 640 --batch 16 --epochs 50 --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_250.yaml --weights yolov5s.pt --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stopped code after 2 models so the output above has partial result of the 3rd training. \n",
    "# 3rd model onwards training is continued in the next cell.\n",
    "# All models were saved in our yolov5 runs train file so results don't require retraining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_500.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_50\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.71 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp24/labels.jpg... \n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Error decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp24\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1248    0.04813          0         43        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.357    0.00577   0.000897\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1246    0.04554          0         45        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00319      0.411    0.00334   0.000764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1241     0.0476          0         56        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00349      0.393    0.00417   0.000883\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1187    0.04686          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00361      0.464    0.00519    0.00125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1141    0.04749          0         43        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00658      0.518    0.00816    0.00189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G     0.1102      0.046          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00713      0.482      0.016    0.00375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G     0.1066    0.06123          0         67        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00919      0.589      0.022    0.00467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G     0.1013    0.06173          0         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00866      0.643     0.0214    0.00463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.09558    0.05501          0         46        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0138      0.589     0.0257    0.00541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.09407    0.05596          0         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0251      0.518     0.0344    0.00794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.09266     0.0601          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0173      0.625     0.0393    0.00957\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.09012    0.06977          0         55        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0538      0.482     0.0614     0.0148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.08621    0.06439          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0391      0.589     0.0759     0.0206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.08474    0.06338          0         50        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.126      0.482      0.119      0.032\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.08344    0.07336          0         56        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.167      0.357      0.147     0.0415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.08671    0.06102          0         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.201      0.319      0.174     0.0482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.07842    0.05551          0         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.278      0.429      0.232     0.0709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G     0.0767    0.05835          0         48        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.277      0.482      0.258     0.0831\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.07896    0.05791          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.293        0.5      0.302      0.102\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.07672    0.06556          0         52        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.421      0.482      0.379      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.07541    0.07307          0         61        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.608      0.472      0.537      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.07175    0.06609          0         59        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.517      0.554      0.567       0.16\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.07598    0.06253          0         55        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.616      0.589      0.605      0.192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.06909    0.06235          0         57        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.619      0.518      0.639      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.07158    0.05666          0         44        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.452       0.56      0.442      0.116\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.07012    0.04761          0         48        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.467      0.563      0.434      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.06818    0.05174          0         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0966      0.732     0.0928     0.0263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.07575    0.05413          0         53        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0966      0.732     0.0928     0.0263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.07064    0.05106          0         48        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.362      0.696      0.327      0.103\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.07087    0.06095          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.114      0.554      0.109     0.0331\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G    0.07433     0.0491          0         42        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.114      0.554      0.109     0.0331\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G    0.06971    0.06218          0         70        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.442      0.536      0.532      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.06217    0.05293          0         52        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.338      0.554      0.416      0.131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.06708    0.04415          0         48        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.338      0.554      0.416      0.131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G     0.0683    0.05096          0         55        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.441      0.482      0.487      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.06567    0.04696          0         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.435      0.592       0.44      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.05538    0.04951          0         54        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.435      0.592       0.44      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.05508    0.04809          0         46        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.444      0.661      0.535      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G    0.05573    0.04605          0         42        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.531      0.668      0.613      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.06083    0.04738          0         50        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.531      0.668      0.613      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G    0.05476    0.04538          0         38        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.591      0.768      0.648      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.05373    0.05002          0         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.526      0.714      0.621      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.05396    0.03877          0         35        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.526      0.714      0.621      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.05903    0.04593          0         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.677      0.748      0.721      0.349\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.05224    0.04927          0         66        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.677      0.748      0.721      0.349\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G    0.04986    0.04365          0         49        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.749       0.75      0.808      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.04776    0.04044          0         42        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.749       0.75      0.808      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.05038    0.04195          0         43        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.774      0.768      0.791      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.05186     0.0473          0         53        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.774      0.768      0.791      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.05287    0.03928          0         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.814      0.804      0.821      0.463\n",
      "\n",
      "50 epochs completed in 0.580 hours.\n",
      "Optimizer stripped from runs/train/exp24/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp24/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp24/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.815      0.804      0.821      0.462\n",
      "Results saved to \u001b[1mruns/train/exp24\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_750.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_75\u001b[0mError decoding JSON from /Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/bhavnasharma/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_75\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/train_750.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.71 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs/train/exp25/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp25\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1238    0.04737          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   WARNING  NMS time limit 1.700s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00333      0.411    0.00525   0.000871\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1245    0.04298          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00347      0.446    0.00355   0.000778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G      0.118    0.04658          0         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00444      0.571    0.00627    0.00148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1109    0.05351          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56    0.00756      0.625     0.0157    0.00361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1029    0.05758          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0182        0.5     0.0232    0.00484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G    0.09707    0.04999          0         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0229      0.482     0.0238    0.00619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G    0.09645    0.05307          0         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0559      0.321     0.0426     0.0116\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G    0.08986    0.06448          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0907      0.159     0.0674     0.0185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.08931    0.05492          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.162      0.232      0.144     0.0391\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       9/49         0G    0.08184    0.05871          0         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.289      0.286      0.235     0.0656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.07898    0.06653          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.402      0.518      0.417      0.135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.07519    0.05558          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.229      0.411      0.229     0.0712\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.07626    0.05996          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0402      0.571     0.0425     0.0123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.07804    0.06082          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.592      0.679      0.587      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.07197    0.05018          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.449      0.567      0.415     0.0987\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.07205    0.04932          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.105      0.536     0.0925       0.03\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G     0.0689    0.04679          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.333      0.518      0.361      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.06433    0.04249          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.391      0.589      0.455      0.165\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.07063    0.05771          0         35        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.347      0.554      0.434      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.06486    0.04763          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56       0.53      0.732      0.601      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G      0.067    0.03785          0         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.452      0.604      0.562      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.06042    0.04112          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.656      0.696      0.761      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.06052    0.03994          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.556      0.732       0.71      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.05555    0.04392          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.585      0.821      0.684      0.283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.06017    0.03729          0         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.488      0.768      0.601      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G     0.0571    0.04167          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56       0.58      0.857      0.682      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.06528    0.03663          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.705      0.854      0.849      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.05365    0.04329          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.406      0.804      0.554      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.06034    0.03588          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.406      0.804      0.554      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.05917    0.04088          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.526      0.813      0.659      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G     0.0522    0.03251          0         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.537      0.857      0.676      0.369\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G    0.05431    0.03419          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.576      0.768      0.719      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.05284    0.03923          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.576      0.768      0.719      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.05089    0.03791          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.605      0.804      0.698      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      34/49         0G    0.04742    0.03467          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.596      0.763      0.719      0.379\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.04998    0.03818          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.654      0.742      0.758       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.04445     0.0321          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.654      0.742      0.758       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.04408    0.03378          0         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.809      0.839      0.911      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G    0.04068    0.03442          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.541      0.839      0.708      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.04801    0.03663          0         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.706      0.857      0.837      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G      0.042    0.03326          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.706      0.857      0.837      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.04178    0.03157          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.568      0.964      0.762      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.03804     0.0317          0         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.826      0.964      0.946      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.03855    0.03101          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.835      0.964      0.938      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.03955     0.0333          0         24        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.835      0.964      0.938      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G    0.03516    0.03115          0         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.993      0.964      0.993        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.03222    0.02888          0         30        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.906      0.964      0.973      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.03644    0.03276          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.982      0.972      0.994      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.03398    0.03148          0         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.982      0.972      0.994      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.03194    0.03332          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.981      0.982      0.994      0.759\n",
      "\n",
      "50 epochs completed in 0.738 hours.\n",
      "Optimizer stripped from runs/train/exp25/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp25/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp25/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.981      0.982      0.994      0.765\n",
      "Results saved to \u001b[1mruns/train/exp25\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 50 --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_500.yaml --weights yolov5s.pt --device cpu\n",
    "!python train.py --img 640 --batch 16 --epochs 50 --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_750.yaml --weights yolov5s.pt --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_100.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp21/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56     0.0323      0.321     0.0298    0.00747\n",
      "Speed: 1.2ms pre-process, 145.2ms inference, 48.9ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp7\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_250.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp22/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.238      0.321      0.187     0.0544\n",
      "Speed: 1.4ms pre-process, 137.8ms inference, 29.2ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp8\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_500.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp24/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.815      0.804      0.821      0.463\n",
      "Speed: 1.2ms pre-process, 135.2ms inference, 9.1ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp9\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data_750.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp25/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.981      0.982      0.994      0.765\n",
      "Speed: 1.2ms pre-process, 138.4ms inference, 1.4ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp10\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/labels/val.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         24         56      0.982      0.952      0.991      0.726\n",
      "Speed: 1.7ms pre-process, 141.7ms inference, 1.5ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Validate the model for each dataset size and capture the output\n",
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp21/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_100.yaml > results_100.txt\n",
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp22/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_250.yaml > results_250.txt\n",
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp24/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_500.yaml > results_500.txt\n",
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp25/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data_750.yaml > results_750.txt\n",
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/data.yaml > results_1000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAABugElEQVR4nO3dd3gUVdvH8e9JL4SE3ntvoRdBFDtFir2D7UGsqI8FUVF8FUV9VLB3pYggagKCoKIREKX3JqEICYSekF7P+8cuuIQEEshmU34frr3YnXNm5p6Z3c2955yZMdZaRERERKR4eXk6ABEREZHySEmYiIiIiAcoCRMRERHxACVhIiIiIh6gJExERETEA5SEiYiIiHiAkjAp94wxDY0x1hjjU4C6txtjFhdTXL2MMduMMUnGmCHFsU45d8aYKGPM3R5ad6AxZrYxJsEY840b13OLMeanoq4rUt4oCZNSxRizyxiTYYypmmv6amci1dBDobkmc0nOxy5jzKhzWOQLwDvW2grW2ogiCrPccR6T9cYYL5dpLxpjvvBgWO5yLVADqGKtvc61wBjzgct7M8MYk+ny+sfCrMRaO9Vae3lR1y0s52fsUncsW6Q4KAmT0mgncNPxF8aYdkCQ58I5RZi1tgKOGMcYY/oWZmaXFrkGwMazCaAgrXrlTG3gRk8HURjGobDf0Q2Av621WbkLrLUjnAl9BWAcMP34a2ttP5f16r0jUkyUhElpNBkY6vJ6GDDJtYIxJtQYM8kYc9AY848x5pnjf9CMMd7GmNeNMYeMMTuAAXnM+6kxZp8xJtbZauJd2CCttX/iSKLaOpd7pzFmszHmqDFmvjGmgcs6rTHmfmPMNmCbMWY70BiY7Wyp8DfG1DbGzDLGHDHGRBtj/uMy//PGmJnGmCnGmGPA7c5usReNMUucy5htjKlijJlqjDlmjFnu2nJojJlgjNnjLFtpjOmda/kznPs00Riz0RjTxaW8njHmO+f+PmyMecelLN/tzrXffzTGPJBr2lpjzNXOhORNY8wBZ3zrjTFtC3E4XgXG5pVgGGP6GGNick070cLi3PZvnPs20bnu5saYp5zx7DHG5G7paWKMWeaMNdIYU9ll2T2cxyTeuX19XMqijDEvGWP+AFJwvAdyx9vKWS/eeRwGOaePBcYANziP910F3TnO7X3SGLMOSDbG+BhjRhljtju3eZMx5iqX+id1yzvfvyOMo/s83hjzrjHGnEVdb2PM/4zjs7nTGPOAKdxQgT+c75N4Y8wOY0xP5/Q9zmM1zKX+AONoQT/mLH8+1/KGGsd3x2FjzLO53hNeLvvnsPOzUdlZFuB8rxx2xrHcGFOjoMdCyhlrrR56lJoHsAu4FNgKtAK8gRgcLQAWaOisNwmIBEKAhsDfwF3OshHAFqAeUBn4zTmvj7P8e+BDIBioDiwD7nGW3Q4szie2hseXAxigF44/pJcAg4FoZ8w+wDPAEpd5LfCzM55A1211qbMQeA8IADoAB4GLnWXPA5nAEBw/rgKBKOc6mwChwCbnfrjUGcMk4HOX5d8KVHGW/ReIAwJclp8G9Hfu85eBv5xl3sBa4E3nPgsAzneWnXa7c+2/ocAfLq9bA/GAP3AFsBIIc+7bVkCtAr5nLNDMOf/dzmkvAl84n/cBYvJ6n+Xa9itc9ttO4GnAF/gPsNNl3iggFkfyHQx8C0xxltUBDjv3oxdwmfN1NZd5dwNtnOvyzRWXr3N/jgb8gIuBRKCFS6xTCrBPTqrn3N41OD4Tx99/1+FoQfQCbgCSj+9zcn0OnPv4B+fxqY/jvdn3LOqOwPE+rQtUAn7B5bOZ3/eBy3qygDtwvCdfdO7Ld3G8hy537qsKLse9nXP7woH9wBCX914ScL5zP7+O4/N1fF0jgb+ccfrj+L6Y5iy7B5iNo3XeG+gMVPT0d6ceJfPh8QD00KMwD/5Nwp7BkQj0xZG8+Di/rBs6v/gygNYu890DRDmf/wqMcCm7nH+TpxpA+vE/RM7ym4DfnM9P+oOSK7aGzuXEA0eBzcBDzrIfcSaBztdeOBK0Bs7XFmdClXtbnc/rAdlAiEv5y/ybSDwPLMw1fxTwtMvr/wE/urweCKw5zb4+CrR3Wf4vLmWtgVTn8/Nw/CE95Q/lmbY7V90QHH/oj++Tl4DPnM8vxpFA9gC8CvmesUBTHInPPzj+qBY2Cfs5135LArxd4rY4uqGP7/dXcu2rDBzvyyeBybnWNR8Y5jLvC6fZlt44kmMvl2nTgOddYj3bJOzOM8yzBhic1+fAuf3nu7yeAYw6i7q/4vzB43x9KYVLwra5lLVzzlvDZdphoEM+y3oLeNP5fAzOpMr5Osh5DI+vazNwiUt5LRxJmg9wJ7AECC/M+1SP8vlQd6SUVpOBm3F88U7KVVYVR4vBPy7T/sHRCgGOX/d7cpUd18A57z5nV0I8jl+51QsRW1VrbSVrbStr7USX5U5wWeYRHC06dVzm20P+agNHrLWJ+WxTfvPvd3memsfrCsdfGGMeM45uwwRnjKE49uVxcS7PU4AAZzdRPeAfm8c4JAq23QA4t20O/47dugmY6iz7FXgHR6vGAWPMR8aYinmsL1/W2rk4Wk3vKcx8Trn32yFrbbbLa3DZl5z6/vLFsS8bANcd3x/OfXI+jj/iec2bW21gj7U2J9fyT9mfZ+Gk9Tq749a4xNmWk98PueV+f1TIr+Jp6ub+bJ5uX+Ql93HCWpvne94Y090Y85txdKEn4GiFO759J8VhrU3BkcAd1wD43mXfbMbxI6kGju+m+cDXxpi9xphXjTG+hdwOKSeUhEmpZK39B0eXUH/gu1zFh3D8Km3gMq0+ji4igH04EgfXsuP24GgJq2qtDXM+Klpr25xjyHtw/MIPc3kEWmuXuG7WaebfC1Q2xoTkijvW5fXp5j8t4xj/9QRwPVDJWhsGJOBImM5kD1A/n3E7BdluV9OAm4wx5+Ho1vzteIG1dqK1tjOOlqXmwOMF3DxXT+PoynM9kSPZ9bVxjP+rdhbLdpX7/ZWJ4325B0dLmOv+CLbWvuJS/0zvg3rm5AH7ud8HZ+vEeo1j3N7HwAM4zrQMAzZQsPfDudiHo4vvuHr5VSwCXwGzgHrW2lDgA/7dvpPiMMYE4uiqP24P0C/XcQyw1sZaazOttWOtta2BnsCVnDyGVeQEJWFSmt2Fowsv2XWis4ViBvCSMSbE+QflUWCKs8oM4CFjTF1jTCVglMu8+4CfgP8ZYyo6B+A2McZceI6xfgA8ZYxpAycG/193hnlct2kPji6Ol50Df8NxbP+U089ZYCE4xtMcBHyMMWOAgrY0LcPxR+sVY0ywM75ezrLCbvdcHMnzCzjO3stxztfV2XLhiyNpSgNy8l9M3qy1UTiSiWEuk//G0ao3wLn8Z3CM8zkXtxpjWhtjgnBsy0zn+3IKMNAYc4VzEHqAcZwYUPf0izthKY6WoyeMMb7GMah/IPD1OcabWzCOpOwggDHmDpwnmLjZDGCkMaaOMSYMR/etu4TgaF1OM8Z0w9GyftxMHMeppzHGD0f3rWsC+gGO75cGAMaYasaYwc7nFxlj2jmT+WM4EvBCv1elfFASJqWWtXa7tXZFPsUP4vhjvQNYjONX72fOso9xdBesBVZxakvaUBzjhjbhGBc1k5O7i84m1u+B8Ti6KI7hSAT6nX6uU9yEY9zZXhwnDzxnrf3lXOJyMR+YhyMh+QdHklOgriBncjEQx7ir3Ti6/G5wlhVqu6216TiOx6U4jtlxFXEct6PO+A4DrwEYY0abwl3n6hkcJ0AcX2cCcB/wCY4WpWTnNpyLycAXOE9uAB5yrmsPjpMVRuNIcPbgaNEr0HextTYDx77uh6Nl7T1gqLV2yznGm3s9m3CMIfwTRxdfO+CPolxHPj7G8SNoHbAaR1KehaOrr6jdB7xgjEnEMQZsxvECa+1GHN8hX+P4gZEEHMDRSg4wAUcr2k/O+f8CujvLauL4zjiGo5vydxzvB5FTGGvPugdDRETEbYwx/YAPrLUNzljZvXFUwHHCTTNr7U5PxiJli1rCRESkRDCO2y71N47rlNUBnsPR6uuJWAYaY4KMMcE4LlGxHsfZmCJFRkmYiIiUFAYYi6PbeTWO7rwxHoplMI6u/704rjN3o1XXkRQxdUeKiIiIeIBawkREREQ8QEmYiIiIiAec8aaoJU3VqlVtw4YNPR1GqZecnExwcLCnw5BzoGNYuun4lX46hqVfcRzDlStXHrLW5nkB6FKXhDVs2JAVK/K7NJQUVFRUFH369PF0GHIOdAxLNx2/0k/HsPQrjmNojPknvzJ1R4qIiIh4gJIwEREREQ9QEiYiIiLiAaVuTFheMjMziYmJIS0tzdOhlBqhoaFs3rwZgICAAOrWrYuvr6+HoxIRESk/ykQSFhMTQ0hICA0bNsQYc+YZhMTEREJCQrDWcvjwYWJiYmjUqJGnwxIRESk3ykR3ZFpaGlWqVFECdhaMMVSpUkWtiCIiIsWsTCRhgBKwc6B9JyIiUvzcloQZYz4zxhwwxmzIp9wYYyYaY6KNMeuMMZ3cFUtx8Pb2pkOHDrRt25brrruOlJSUc17mmDFj+OWXX/It/+CDD5g0adI5r0dERESKnzvHhH0BvAPklyX0w3Fn+mZAd+B95/9uF7E6ltfmb2VvfCq1wwJ5/IoWDOlY55yWGRgYyJo1awC45ZZb+OCDD3j00UdPlGdlZeHjU7jd/cILL5y2fMSIEYWOU0REREoGt7WEWWsXAkdOU2UwMMk6/AWEGWNquSue4yJWx/LUd+uJjU/FArHxqTz13XoiVscW2Tp69+5NdHQ0UVFR9O7dm0GDBtG6dWuys7N5/PHH6dq1K+Hh4Xz44Ycn5hk/fjzt2rWjffv2jBo1CoDbb7+dmTNnAjBq1Chat25NeHg4jz32GADPP/88r7/+OgBr1qyhR48ehIeHc9VVV3H06FEA+vTpw5NPPkm3bt1o3rw5ixYtKrLtFBEpCebsmMPlMy8n/MtwLp95OXN2zPF0SCIF4smzI+sAe1xexzin7TuXhY6dvZFNe4/lW756dzwZ2TknTUvNzOaJmeuYtmx3nvO0rl2R5wa2KdD6s7Ky+PHHH+nbty8Aq1atYsOGDTRq1IiPPvqI0NBQli9fTnp6Or169eLyyy9ny5YtREZGsnTpUoKCgjhy5OTc9fDhw3z//fds2bIFYwzx8fGnrHfo0KG8/fbbXHjhhYwZM4axY8fy1ltvnYhp2bJlzJ07l7Fjx562i1NEpDSZs2MOzy95nrRsx8lF+5L38fyS5wEY0HiAByOTkmzOjjlMWDWBfcn7qDWzFiM7jfTI+6VUXKLCGDMcGA5Qo0YNoqKiTioPDQ0lMTERgMyMTLKzs/NdVu4EzHV6fvNlZmSeWH5+UlNTCQ8PB+C8887j+uuvZ+nSpXTu3JmqVauSmJjI3Llz2bBhAzNmzADg2LFjrF27lgULFnDTTTeRnZ1NYmIivr6+JCYmkpmZSWpqKl5eXvj5+TF06FD69u1L3759SUxMJD09HV9fX2JiYjh69CidOnUiMTGRa665hmHDhpGYmEh2dvaJ+i1atGDHjh0nprtuU1pa2in7VUq2pKQkHbNSTMevaIyPGX8iATsuLTuNl/54iZitMXgZLwyOk4+88DpxIpLB4OXsDDLGcMq/AkxLSUlh1oJZedY7sT7nuo1xWV+uelK8lictZ9qRaWTaTMCRuD+7+Fk2bdpE1wpdizUWTyZhsUA9l9d1ndNOYa39CPgIoEuXLjb3zTY3b95MSEgIAC9e0+G0K+31yq/ExqeeMr1OWCAz7zu/oLGfIjAwkHXr1p00LSgoiIoVK56Izdvbm3fffZcrrrjipHqLFi0iICDgRL3jfH19CQwMpFKlSqxYsYIFCxYwc+ZMPv30U3799Vf8/f3x9/cnJCQEY8yJ+StUqICXlxchISF4e3tTqVIlQkJCSE9PJycnh5CQkBPXCTsuICCAjh07nvX2S/HTzYNLNx2/c3cg5QBH/zmaZ1liTiLvHHjH/UHkvfoC8zJejuTMOJI2L+N1ItnL/dzLeJ06j/N5fvMcT/aOPz9efmIe1+d5lee1zLOI6aR6uZ6fiPEsYjppu3Jta17L9DJeRK6MPJGAHZdpM/k57Wcev/LxczugheTJJGwW8IAx5mscA/ITrLXn1BVZEI9f0YKnvltPaua/rV6Bvt48fkULd6+aK664gvfff5+LL74YX19f/v77b+rUqcNll13GCy+8wC233HKiO7Jy5con5ktKSiIlJYX+/fvTq1cvGjdufNJyQ0NDqVSpEosWLaJ3795MnjyZCy+80O3bIyJS3NKz0/ltz29ERkeyZO+SfOtVCajC6xe+jsWSY3P+/d/afJ/ncPL/Jz3PY54tW7fQvHlzcmzOiXVYa09+jqMMOH29PJ6fbpmn1Mtvu1zmzyEHLCdiym/d2TnZZJN98jzO5/ktO8/Y89m/hTkmrnXdLS45zu3ryM1tSZgxZhrQB6hqjIkBngN8Aay1HwBzgf5ANJAC3OGuWFwdPwuyqM+OLIi7776bXbt20alTJ6y1VKtWjYiICPr27cuaNWvo0qULfn5+9O/fn3Hjxp2YLzExkcGDB5OWloa1ljfeeOOUZX/55ZeMGDGClJQUGjduzOeff+727RERKQ7WWjYd3sT30d/z484fOZZxjJrBNbm73d3E/72RWSmLSPP69zyzgJwcrvbqRJeaXdwaV9TeKPq06OPWdci/8k0mXZK1giS810TeRGLW4VOWX9G3WnFvEsZa92eXRalLly52xYoVJ03bvHkzrVq18lBEpVPu7kjtw9JH3Vmlm47fmR1KPcScHXOIiI4gOj4af29/Lql/CUOaDqFbzW54e3kT93xTVgYnM6FSGHE+3tTMymbk0Xh6JnvzT+/XyfQNJdOvIpm+jkeOt/+JNhXHnz/r8pxcZTj/cOcuczzbtOnf702bezn21Pon/tqedtn5l5FrOdaeumzXeXL/fT/9Nv1b5vr6pPhPWnbe9W0+ceaum18Zee23AqzXNdYzrTdy22xMtZkYr3+7JG2OL4EJN7L84ScoasaYldbaPH8RlIqB+SIiUj5kZmeyMGYhEdERLIpdRLbNJrxaOGPOG0Pfhn0J8XP+eMxI4eCyGdSwBxmQDAOST71AdqVF95wyLc36kkAwx2wwCQST4PL/MYJOmp67Tgr+QK7B9OvWFP1OKCOM+Xdv/XtChGuZ89XJ/51UZk4pc1lOfmVnWG9yUnt8snLwrzYf4xuPzQwj/eAVJB8r2FUQipKSMBER8bitR7YSER3BnB1zOJp+lGqB1RjWZhiDmw6mcahzHKy1JGxfRtxvH1I3di7VbAqZeOHLqWe9H6ASCYM+xzs9AZ+MROf/CXhlJOCbnkD1jARqpSfgnX4M74zdeKcn4JVxDHOasUfWy4ccv4pk+4eS4x/G0TRLSPX65PiHkhMQ5vjfPxQbEOp8HnbitfWvCMbr36TCJTn4N9HIO/HgDEmJa4LhuhzMGZIY8k54OE3ZaddbSs72dJyg15GsYyefjFYnLLDYY1ESJiIiHnE07Shzd84lMjqSzUc24+vly0X1LmJI0yGcV/s8fLwcf6LSjh1i+4LPqLh5GvUyduBn/Vjsdz6pbW8iNPMA3daPJdBknFhuqvXjn85P0bXTRYULKCcH0hMgLQFS4yEt3vl/AqTFY1Lj8U6Lx9s5LTh1NxWObHTUS0uAnKzTLNxAQEUICIWAMAgMc/wfEPrv8xPTwnJNCwVv38Jti+TLkyfo5aYkTEREik1WThZ/xP5B5PZIftvzG1k5WbSu0prR3UfTr2E/wgLCAMjJzmbTH7PIWPYlrRJ+pw2ZbDJN+LHhEzTqM4zLGtY9sczlfr7UW/Ua1e0hDpiq7On8OF0HndoVeUZeXhBYyfGodObqq1zH9VkLGcmnJG4nkrlTErt4OPT3v8+z0k5dgSvf4DMkbrmnuSR7vsXfwlOSuZ6gFxufSp1iPEEvNyVhIiLidtvjtxMZHcnsHbM5lHqIygGVubnlzQxuOpjmlZqfqLdzx9/E/PYJjWMiaG33k2CDWVZ5IMHn3UH7Lr1p7XVql1fXQfeAM+mq6XwUO2PAv4LjEVr3zPVzy0wrQOLmUh6/G1LXOaZlnP5i4nj7nz5Jy29aQCj4h5zaT1kGDOlYhyEd63j8BBklYSIi4hbHMo4xb+c8IqIjWH9oPT7GhwvqXsDgpoPpXbc3vl6OLrZDCUmsXfA1FTdPo1PGShoZy0b/Duxt+xhtLrmZ3kEVPLwlxcA3wPEIqVH4ebOz/k3QXJO4/Frkkg64tMIlwGnGwWG8XRK4gnSlhp783Mu78NtTjigJKyLe3t60a9eOrKwsGjVqxOTJkwkLCyuy5Tds2JAVK1ZQtWpVKlSoQFJSUpEtW0SkqGTnZLN031IioiNYsHsBGTkZNKvUjMe7PM6AxgOoElgFgNSMbBYuXUTG8i/pmjCfS8wxDpkqrGt0F/Uu+Q9t6rX08JaUIt4+EFzF8SisnBxHS9qZEjfX8oSYf5/nZOazYCf/is4kLbRwY+ACwsDHr/DbU1DrZsCCF7gwIQZW14VLxkD49e5bXz7KZxLm3PkkxDiajYtg5wcGBrJmzRoAhg0bxrvvvsvTTz9dBMGKiJR8/xz7h8joSGZtn8X+lP2E+odyTfNrGNJ0CK0qt8IYQ3aO5a8tu9m18Cuax37HJWYrWXizvXJv0s+7gzpdBlJVLSfFy8vLmfSEAg0KN6+1kJlSwDFwzvLD2/8tzzz1siIn8Q0683i3/Kb5BubfjbpuBsx+CDJTHWd4JuxxvIZiT8TKXxLmsvMBt+z8884778R9JLdv387999/PwYMHCQoK4uOPP6Zly5bs37+fESNGsGPHDgDef/99evbsyZAhQ9izZw9paWmMHDmS4cOHF0lMIiJFLTkzmfm75hMRHcHqA6vxMl70qt2LJ7o+QZ96ffDzdrRkbN13jD8X/0To5q+5NHsxPUwqB/zrs6vtk9TvcxctKp5FF5x4njHgF+x4hJ7FoPas9JOTtbxOXnBN4o7FwH7n2ajpx06/bG+/XImZy/N1M/7NAY7LTHU0zigJO0c/joK49fmXxyyH7PSTp2WmQuQDsPLLvOep2Q76vVKg1WdnZ7NgwQLuuusuAIYPH84HH3xAs2bNWLp0Kffddx+//vorDz30EBdeeCHff/892dnZJ7oXP/vsMypXrkxqaipdu3blmmuuoUqVs2hiFhFxgxybw4q4FURER/DL7l9IzUqlUWgjHun8CAMbD6RakOPWLweOpTFv+SpSVnxFn+R53O61h3QTwP4G/fDr8x+qN+pZJgd8SyH4+EOF6o5HYWVnORKxM52BenxaymFnK1xC/glcQszZbcc5KHtJ2JnkTsDONL2AUlNT6dChA7GxsbRq1YrLLruMpKQklixZwnXXXXeiXnq6Yz2//vorkyZNAhzjyUJDQwGYOHEi33//PQB79uxh27ZtSsJExONiEmOYtX0Ws7bPIjYplhDfEK5sfCVDmg6hXdV2GGNITs/i+1W7+fvPH2gdF8kNXivwN1kcDGtLYveRhHS+gfoBFT29KVIWePtAUGXHo7DebOvoBcvtbM5qPUdlLwk7U4tVvju/Htwx56xXe3xMWEpKCldccQXvvvsut99+O2FhYSfGip1JVFQUv/zyC3/++SdBQUH06dOHtLQzXDtGRMRNUjJT+GX3L0RER7A8bjkGQ49aPXio40NcXP9iAnwCyM6xLNp2iN+Wraby3zMYQhRXeR0k1b8iqa2H4d/rLqrVKP7bwYjk65IxJw9LAscYskvGFHsoZS8JOxM37/ygoCAmTpzIkCFDuO+++2jUqBHffPMN1113HdZa1q1bR/v27bnkkkt4//33efjhh090RyYkJFCpUiWCgoLYsmULf/31V5HEJCJSUNZaVh9YTUR0BPN3zSclK4X6IfV5sOODDGw8kFoVamGtZdO+Y0Su2ErC2ln0y/iZZ73X4eVlSajVi5ye4wlsOYBA3wBPb47IqY6P+1rwAjYhBlNEJ+idjfKXhLns/KI8O9JVx44dCQ8PZ9q0aUydOpV7772XF198kczMTG688Ubat2/PhAkTGD58OJ9++ine3t68//779O3blw8++IBWrVrRokULevToUWQxiYicTlxyHLO2zyIyOpLdibsJ8gniioZXMLjpYDpV74Qxhn0JqbwftZ1VK/6kW/wc7vFeTBVzjNQKNcnp/F+8Ot9GaKWGnt4UkTMLvx7Cr+d3XazVA5w7vyjlvm7X7NmzTzyfN2/eKfVr1KhBZGTkKdN//PHHPJe/a9eufNclInI20rLS+HX3r0Ruj+TPvX9isXSt2ZXh4cO5rMFlBPkGkZiWycyVMcxdGU213XO5wfs37vXaRravD9nN+kLXOwhscpEuyilyFspnEiYiUk5Za1l/aD2R0ZH8uPNHEjMTqR1cm3va38OgJoOoF1KPrOwcFm07xHertnBg82KG2F951+dPgnzTyKjUDLq+iHf4jXhXqObpzREp1ZSEiYiUAwdTDjJ7x2wioyPZkbCDAO8ALm1wKUOaDqFrza4YDOtjE/j0t40sWruFPmm/MtL3d5p67yHbJwivttdCp6H41eumS0uIFBElYSIiZVRGdgZRe6KI3B7JH7F/kG2z6VCtA8+f9zxXNLyCCn4ViDmawnu/bSdy1W5qHVnKjT6/84zXCnx8s8ip0xU6PY5326sdN3IWkSKlJExEpAyx1rL5yGYioyOZs3MOCekJVA+qzh1t72Bwk8E0DG1IQmomP6zZx3er1xO7cyvX+fzOV36LqeZ3gJzAyni1vwc63YZX9Vae3hyRMk1JmIhIGXAk7QhzdswhIjqCv4/+jZ+XH5fUv4TBTQfTo1YPsnMMv/99kFd/WMnvm2O5MGc5jwcupEvAWgBMo4ug01C8WvR3XMlcRNxOSZiISCmVmZPJ4pjFRERHsDBmIVk2i3ZV2/FM92fo26gvFf0qsnpPPM/P2swP6/ZSLXUHwwIW8rr/YoKyE7AV6mI6PAkdb4Gw+p7eHJFyR0mYBwwZMoS4uLiTLsb6/PPP8/HHH1OtWjWysrIYN24cgwYN4osvvuDxxx+nTh3HzVEfeOAB7r77bgC+/PJLXnzxRQCeeeYZhg0bVvwbIyLFbtvRbURER/DDjh84knaEKgFVuLX1rQxuMpimlZryz+Fkvli0l4g1qzlw6BBX+f7Fd8GLaZSzGWt8MS0GQKehmMZ9dGkJEQ8ql0nYnB1zmLBqAnHJcdQMrsnITiMZ0HhAsaw7Pj6elStXUqFCBXbs2EHjxo1PlD3yyCM89thjbN68md69e3PgwAEAbrjhBt55552TlnPkyBHGjh3LihUrMMbQuXNnBg0aRKVKlYplO0SkeCWkJzB351wioiPYdHgTPl4+9KnbhyFNh9CrTi+S0nL4Yd0+nly9hJX/HKGT2caYsCWcH7wI3+xUCGkFF47DhN8AwVU9vTkiQjlMwubsmMPzS54nLdtxT8Z9yft4fsnzAOeUiO3atYu+ffvSo0cPlixZQteuXbnjjjt47rnnOHDgAFOnTqVbt2589913DBw4kBo1avD1118zevToU5bVqlUrfHx8OHToUL7rmz9/PpdddhmVKztuXnrZZZcxb948brrpprPeBhEpWbJzslmydwkR0RH8tuc3MnMyaVm5JaO6jaJ/o/4E+VTkty0HuO+3Nfy29QAVs+MZHrqMTyr/RqWUXZBdAdpfBx2HQt0uurSESAlT5pKw8cvGs+XIlnzL1x1cR0ZOxknT0rLTGPPHGGb+PTPPeVpWbsmT3Z4847qjo6P55ptv+Oyzz+jatStfffUVixcvZtasWYwbN46IiAimTZvGmDFjqFGjBtdcc02eSdjSpUvx8vKiWjXHhRC//fZbFi5cSPPmzXnzzTepV68esbGx1KtX78Q8devWJTY29owxikjJtyNhB5HRkfyw/QcOpB6gkn8lbmhxA4ObDqZFpRas+Ocor/0Yw5x1+0hMTefKoM3Mqr6ElvGLMOlZULcbXPoYtLkK/Ct4enNEJB9lLgk7k9wJ2JmmF0ajRo1o164dAG3atOGSSy7BGEO7du3YtWsX+/fvZ9u2bZx//vkYY/D19WXDhg20bdsWgDfffJMpU6YQEhLC9OnTMcYwcOBAbrrpJvz9/fnwww8ZNmwYv/766znHKiIlS2JGIvN2zSMiOoJ1B9fhbbzpXac3o5uO5oK6F7DnSDoRK2L5z5rf2HMklSa+hxhffQV9Un8iICUO0qtAjxHQ8Tao3tLTmyMiBVDmkrAztVhdPvNy9iXvO2V6reBafN7383Nat7//v6d1e3l5nXjt5eVFVlYWM2bM4OjRozRq1AiAY8eOMW3aNF566SXg3zFhrqpUqXLi+d13380TTzwBQJ06dYiKijpRFhMT49GbkIpI4eXYHJbuW0pEdAQLdi8gPTudpmFNeazLYwxoPACTHcIP6/Zx7Q/LWLsnHn+TwYO1tnJd3ShqHPoTDhtoegl0ehWa9wMfP09vkogUQplLws5kZKeRJ40JAwjwDmBkp5FuX/e0adOYN28e5513HgA7d+7k0ksvPZGE5WXfvn3UqlULgFmzZtGqlePiiVdccQWjR4/m6NGjAPz000+8/PLLbt4CESkKe47t4Yf4H3jp25eIS44jxC+EIU2HMKTpEJpUbMmCLQd4cvpOfv/7IFk5ln7VDvFi0z9pc2geXkeOQmh96DMaOtwMYfXOvEIRKZHKXRJ2fPB9cZ8duWvXLkJDQ+nRo8eJaY0aNSI0NJSlS5fmO9/EiROZNWsWPj4+VK5cmS+++AKAypUr8+yzz9K1a1cAxowZc2KQvoiUPCmZKczfNZ/I7ZGs3L8Sg6FnnZ78t/N/ubBuH9buSWZyVCxz1y8gMT2LxiHZvN18A32S5xF4cC2k+EHLK6HTbdCoD3h5eXqTROQcGWutp2MolC5dutgVK1acNG3z5s0nWoikYBITEwkJ+fdecNqHpU9UVJS6oEs4ay0r9q8gMjqSn/75idSsVBpWbMjgpoOpur8qbcMv4btVsUSu2UtsfCpBfl7c2/gg13v9RvU98zCZKVC9NXQaCuE3QJB+aJUk+gyWfsVxDI0xK621XfIqK3ctYSIi7rY3aS+zts8iMjqSmKQYgn2D6d+oP0OaDqF2QEtmr9vHm8u28M8vC/EyMKCxDx81WUmrfZF47YoGvxAIv95xaYk6nXRpCZEySkmYiEgRSM1KZcHuBURER7Bs3zIslu41u3Nfh/s4v9ZFLPw7gbfmxLJo269k51gah1g+6nGYC5LmEbDjJ4jNgno9oPej0GYI+AV7epNExM2UhImInCVrLWsPriUiOoL5u+aTlJlEnQp1uLfDvVzZaBC7D/jz3cpYRm1YTHJGNrVDA3i8mz/Xe0URvGEK/muOQHA16HGf49IS1Zp7epNEpBiVmSTMWotRk/1ZKW3jAkU8bX/yfmbvmE1kdCS7ju0i0CeQyxpcxpCmQwi2zYhcvY/r5mwh7lgaIf4+DG5bmTsqb6BpzPeYNQvBeHG4Ukf8L5sIzfuCt6+nN0lEPKBMJGEBAQEcPnyYKlWqKBErJGsthw8fJiAgwNOhiJRo6dnp/LbnNyKiI/hz75/k2Bw6Ve/EnW3vpEPlC/h5Yzxjpu9l874/8PEyXNi8GuPPN/Q6NhefDd/ApngIawAXPQMdbmb96m30adXH05slIh5UJpKwunXrEhMTw8GDBz0dSqmRlpZ2IvEKCAigbt26Ho5IpOSx1rLp8Ca+j/6eH3f+yLGMY9QMrsnd7e7msnoD2PiPH98tjOW/0X+RY6F9vTDG9avPIO8lVNg4Hn5dA97+0Gqg49ISDS9wubTENk9umoiUAGUiCfP19T1xFXopmKioKDp27OjpMERKpEOph5izYw4R0RFEx0fj7+3PJfUvYWDjQWQmNyFy9T7e++5vUjOzqVspkPv7NOHGmjHU2T4JFkVCVirUaAv9XoV21+nSEiKSpzKRhImInKvM7EwWxiwkIjqCRbGLyLbZhFcL59kez9LAvyc/bTjGI1/u5WDiSioG+DCkYx2ub+lH+8Nz8Vr9GPy5HfwrQoebHIPsa3fUpSVE5LSUhIlIubb1yFYioiOYs2MOR9OPUi2wGkPbDKVn9b6s2u7HJ3Nj+Xv/Wny9DRe1qM41HWpwkfc6/Na9DN/MA5sN9XvCBY9D68HgF+TpTRKRUkJJmIiUO0fTjjJ351wioiPYcmQLvl6+XFTvIi6vfyVHDzcicmUcb+/cibXQqX4Y/zekLYPqphK6ZTr89BUkxUFwdej5gKPVq2ozT2+SiJRCSsJEpFzIysnij9g/iIiOIComiqycLFpXac0TXUcRlt2V+euTeOiX/aRnbaRBlSBGXtKMq9tVpn7cAlj9MsxbBMYLml3uuI1Qs8t1aQkROSdKwkSkTNsev52I6Ahmb5/N4bTDVA6ozE0tbqJVyMWs3BbIW9/u5XDyNsKCfLm+Sz2u6lSHjj7/YFZ/CJ99A+kJUKkRXPwsdLgZKtb29CaJSBmhJExEypyE9ATm7ZxH5PZI1h9aj4/xoXfd3pxfsx/79jUkctF+3ju4Hz9vLy5tXZ0hHerQp4E/fptmwtxJELfOcWmJ1oMdl5ZocL7LpSVERIqGkjARKROyc7L5a99fREZHsmD3AjJyMmhWqRkPtn8Ur5TO/LQumSd+OgLsoFvDyvynd2P6t61J6P6lsOpZ+H4WZKVBzXbQ/3Vody0EVvL0ZolIGaYkTERKtV0Ju5i1fRazts9if8p+Qv1Duarp1dT2voBlWwN5/ZuDZGTvoXG1YB67vDmDO9Shnk8CrP0KPp4MR3eCfyh0vNV5aYkOnt4kESknlISJSKmTlJHET//8RER0BKsPrMbLeNGrdi+ubXg//8Q0YObPh4hPOUaV4DRu7l6fqzvVoV3NIEz0zzDvWdj2k+PSEg3Ohz5POa5or0tLiEgxUxImIqVCjs1hRdwKIqIj+GX3L6RmpdIotBG3t7yftPgO/Lw6lbmHU/D32c/lbWpyVcfa9G5WDd/4nbDqTfh6GiTthwo1oNdDjlavKk08vVkiUo4pCROREmXOjjlMWDWBuOQ4agbX5NZWt5KUmcSs7bOITYqlgm8FLqvfn4pZ5/HX5mDe/isBYw7To1EV7ruoKf3a1iTEKxM2RcKkyfDPH2C8ofkVjktLNL0MvPXVJyKep28iESkx5uyYw/NLnictOw2Afcn7eG3FawB0r9mDC6rexvZdjZgxL57M7Cya18jmyb4tGdyhNrVDA2DfGvjlDVg/E9KPQeXGcMlzjktLhNT04JaJiJxKSZiIlBgTVk04kYC5CjCVWPbntfySlkW1kBSGndeQIR3r0KZ2RUxaPKybDKsmwf714BMArYc4Ly3RS/dvFJESS0mYiJQY+5Lj8pyemnOUS1tW56pOdenVpAo+Bti1CL6bDJtmQXY61OoAA/4Hba+FwLDiDFtE5KwoCRORkiM7CLyTT5nslV2Jt27sCMf2wh//g9VT4OguCAiFzsMcg+xrhRd/vCIi50BJmIiUCLuP7caaNIwFXHoQvXK8aHKgKUy9HqJ/BpsDDXvDRc9AqyvBN9BjMYuInAu3JmHGmL7ABMAb+MRa+0qu8vrAl0CYs84oa+1cd8YkIiVPZnYmTy58El/rxUOHjzAtLJg4H29qZmXz0JF4rkz/FuJqwfmPOC6qWrmxp0MWETlnbkvCjDHewLvAZUAMsNwYM8tau8ml2jPADGvt+8aY1sBcoKG7YhKRkumdNe+w4fAGnj6Yzo2pCdyRlHBSeZpfZQIe3qBLS4hImeLOO9J2A6KttTustRnA18DgXHUsUNH5PBTY68Z4RKQE+mvfX3y+4XOy4rtxQ+qBPOsEZBxVAiYiZY47v9XqAHtcXscA3XPVeR74yRjzIBAMXJrXgowxw4HhADVq1CAqKqqoYy13kpKStB9LubJwDBOzExm39xVsRlUqHxtAttdsfHLST6mX5l+Vv0r5tuZWFo5feadjWPp5+hh6+qflTcAX1tr/GWPOAyYbY9paa3NcK1lrPwI+AujSpYvt06dP8UdaxkRFRaH9WLqV9mNoreX+BQ+SlJ0CB+7kx+bz8dmcDl6+kJP5b0XfQAIGjKNPeB+PxeoOpf34iY5hWeDpY+jO7shYoJ7L67rOaa7uAmYAWGv/BAKAqm6MSURKiGlbprEo9nfS9vdlTsM1VNz8NVz4JAx5D0LrAcbx/8CJEH69p8MVESly7mwJWw40M8Y0wpF83QjcnKvObuAS4AtjTCscSdhBN8YkIiXA1iNbeXX562QltWBSlVTqb5sEPe6HPk85rnCvpEtEygG3tYRZa7OAB4D5wGYcZ0FuNMa8YIwZ5Kz2X+A/xpi1wDTgdmutdVdMIuJ5qVmpPLTgMbIy/XkyuybnxXwOnYbBFS/pFkMiUq64dUyY85pfc3NNG+PyfBPQy50xiEjJ8sKS8exN2UXf+C7cnjgZ2l0HV76pBExEyh13jgkTETnJvJ0/88POb2kc35jXE7+DFv1hyPvg5e3p0EREip2SMBEpFnHJcYxeNIbgtDC+OboQGl0I134O3r6eDk1ExCM8fYkKESkHsnOyuWvuo2Rnp/HVoVj86nWFm6aBb4CnQxMR8RglYSLidi/+8S67U9bz3OEEGlVpATfPAL9gT4clIuJRSsJExK1+/2c5M7d/wmXJaQzxr4657XsIDPN0WCIiHqcxYSLiNkdSE3j810eolZXF0+kB+AybDcG6HrOICKglTETcxFrLnd8+QLqJ5/8Ssqly549QsZanwxIRKTHUEiYibvHSvLfYnr2Gu4+l033obKjUwNMhiYiUKGoJE5Ei99u6KCLjPqVjRhb/uXoGVGvu6ZBEREoctYSJSJE6eGAPE/56AD9rGX3eawTU7ejpkERESiQlYSJSZLJTE3h7xkC2+xvurT+UluEDPR2SiEiJpSRMRIpGRgo/fNyP70OyOd+/A7de+qSnIxIRKdGUhInIuctKZ9snV/FWcDxVbRhvXPuRpyMSESnxNDBfRM5NdhbHpt7GW2Y7R72DmTLgIwJ9Aj0dlYhIiaeWMBE5ezk5ZHx3L7MP/8HCoEBGtH+UttVaeToqEZFSQUmYiJwda7Fz/svObd/zaqUqtK98Hvd0GOrpqERESg0lYSJSeNbCz2NIW/U5d1drSKBvJd6+bDzGGE9HJiJSamhMmIgU3sLXYMlE7q7cjni/Y3x08dtUCqjk6ahEREoVtYSJSOH8+R789hLjAruwLjSBoa1v57za53k6KhGRUkdJmIgU3MovYf5TRPp3Y1q1RJqHtebhzg96OioRkVJJSZiIFMz6mdjZI1kX1JXRYQH4+8JbF72Or5evpyMTESmVlISJyJltmQvfDWdvaCdu8GqDV9Aunuv5LPUq1vN0ZCIipZaSMBE5vR1R8M3tHKvUhr6JV+JTLYoBjQYwsInuCykici6UhIlI/nYvhWk3kRHWmH7x9+BdN5I6FWrzTI9nPB2ZiEippyRMRPK2dw1MvY6ckFrcljmKY1Xng08Cr14wngp+FTwdnYhIqackTEROdWALTLkaGxDC2LBxrMpYC8FreKDjA4RXC/d0dCIiZYKSMBE52ZGdMHkIePnwTet3mRQdR4XaP9CtZjfuaHOHp6MTESkzlISJyL8SYmHSIMhKY02fz3lq4TFqNJlJBb8Axp0/Dm8vb09HKCJSZui2RSLikHTQ0QKWcpQD13zDnTOSqFpvAcn8w4SeE6gRXMPTEYqIlClKwkQEUuNhylUQv4f0m2Zy19xsMn03kxn4Kze0uIGL61/s6QhFRMocdUeKlHfpSTD1Wji4FXvDFJ5eFcKGuFhC6s2kaVhTHuvymKcjFBEpk5SEiZRnmWnw9U0Quwqu/Ywph5oyc+VuWrSbQ4ZN4dULXiXAJ8DTUYqIlElKwkTKq+xM+GYY7FwEQ95neWAvxs7eROuW64hNX81jXR6jWaVmno5SRKTMUhImUh7lZMN3w+HveXDlG+xvNJj7pq6iZrVDxHl/y0X1LuKGFjd4OkoRkTJNSZhIeZOTA7Mfgo3fwWX/R0aH27l3ykqSM1IIrjedSv6VGNtzLMYYT0cqIlKmKQkTKU+shflPweopcOGT0Oshxs7eyKrd8fTouph9yXt4uffLVAqo5OlIRUTKPCVhIuXJby/B0g+gx33Q5ymmL9/N1KW76df9AMsP/8hd7e6iW61uno5SRKRcUBImUl4sfgsWvgadhsIV41gTk8CzERvp3sywNu1jwquGc1+H+zwdpYhIuaEkTKQ8WPYx/PIctL0WrnyLQ8kZ3DtlJdUq+uBTYxo5NodXLngFXy9fT0cqIlJuKAkTKevWTIO5j0GL/nDVB2Raw/1TV3EkOYPLem5g3eE1PNPjGeqF1PN0pCIi5YqSMJGybFMkRN4HjS6Eaz8Hb1/Gzd3M0p1HuLev4fudXzCw8UCubHylpyMVESl3lISJlFXbfoaZd0HdrnDTNPAN4PvVMXz+xy5uOa8qc+L+R50KdXi6x9OejlREpFzSDbxFyqJdi2H6rVC9Fdw8A/yC2RCbwFPfradbo0qkVJzOoT2HmNx/MsG+wZ6OVkSkXFJLmEhZE7MSvroBwhrAbd9DYBhHkzMYMWUlYYF+9D9vD7/s/pkHOj5A26ptPR2tiEi5pSRMpCzZvxGmXA1BVWBoBARXJTvH8tDXqzlwLJ0xV1flvfX/o3ut7tzR9g5PRysiUq6pO1KkrDgUDZOGgG8QDJsFFWsD8Nr8rSzadoiXrmrJ59tG4e/tz7jzx+Fl9BtMRMSTlISJlAXxu2HSYLA5MDQSKjUEYM66fXzw+3Zu7l6fGDOTLUe28PbFb1M9qLpn4xUREXVHipR6iXGOBCwj0TEGrFpzALbGJfL4zLV0qh/GpZ2OMGXzFG5qeRN96vXxbLwiIgKoJUykdEs54uiCTNzvaAGrFQ5AQmom90xeQbC/Dy9d24ARv91Cs0rN+G+X/3o2XhEROUFJmEgp5Z2V4hiEf2QH3PIN1OsKQE6O5ZHpa4g5msrU/3TjrbVPk5yZzGdXfIa/t7+HoxYRkePUHSlSGmWk0G79/0Hcerh+EjS+8ETRWwu28euWA4wZ2JotKXNYsncJT3R9giZhTTwYsIiI5KYkTKS0yUqH6bcSmrAFrv4IWvQ9UfTzpv1MXLCNazvXpXOzZN5a9RYX17uY65pf58GARUQkL25NwowxfY0xW40x0caYUfnUud4Ys8kYs9EY85U74xEp9bKzYOadsH0BW1vcD22vOVG0/WASj05fQ7s6oYwe0JgnFz1J5YDKjO05FmOMB4MWEZG8uG1MmDHGG3gXuAyIAZYbY2ZZaze51GkGPAX0stYeNcbovHmR/OTkQOT9sOUH6DueuLSWtHQWJaVncc/klfj6ePHBbZ15c/U4dh/bzadXfEpYQJgnoxYRkXy4syWsGxBtrd1hrc0AvgYG56rzH+Bda+1RAGvtATfGI1J6WQtz/wvrvoaLn4EeI1yKLI/NWMvOQ8m8c3NH1h/9nYjoCO5udzdda3b1YNAiInI67kzC6gB7XF7HOKe5ag40N8b8YYz5yxjTFxE5mbXw8xhY8Rn0ehh6P3ZS8XtR25m3MY6n+rWkQY10xv45lvBq4dzb4V7PxCsiIgXi6UtU+ADNgD5AXWChMaadtTbetZIxZjgwHKBGjRpERUUVb5RlUFJSkvZjKdFg13Qa7fqK2Nr92ebTB37/HXAcw4nf/MKbK9PpXtObhpk7uPeHiWRlZXGV71X8sfAPzwYup6XPYOmnY1j6efoYujMJiwXqubyu65zmKgZYaq3NBHYaY/7GkZQtd61krf0I+AigS5cutk+fPu6KudyIiopC+7EU+PM92PUVtL+JOoPfo47Xv43XM+b+yqebsmhRM4TP7+vJZxs/YOeenYzvPZ7+jft7MGgpCH0GSz8dw9LP08fQnd2Ry4FmxphGxhg/4EZgVq46EThawTDGVMXRPbnDjTGJlB4rv4T5T0GrQTDoHXBJwFIysnh7dToAH93WhU1H1vDx+o8Z1GSQEjARkVLCbUmYtTYLeACYD2wGZlhrNxpjXjDGDHJWmw8cNsZsAn4DHrfWHnZXTCKlxvqZMHskNL0UrvkUvP9ttLbWMurb9cQk5jDhxg6EVshk1KJR1K1Ql9HdR3swaBERKQy3jgmz1s4F5uaaNsbluQUedT5EBGDrj/D9PdCgJ1w/GXz8Tir+dPFOZq3dyzXNfLmweTUejXqUw2mHmdJvCsG+wR4KWkRECktXzBcpSXZEwYxhUDMcbvoa/IJOKl6y/RAv/7iFvm1qcmVjX2Zum8kvu3/hoY4P0aZqG8/ELCIiZ0VJmEhJsXspTLsJqjSFW7+FgIonFcfGp/LAV6tpVDWY169vT1xmHK8ue5Xzap3HsDbDPBS0iIicLSVhIiXB3jUw9ToIqQVDIyCo8knFaZnZjJi8ksysHD68rTO+Ptl8cegLAn0Ceen8l/Ay+iiLiJQ2nr5OmIgc2AJTrna0fA2NhAon373LWsszERtYH5vAx0O70KRaBV5Z9gp7M/fy7iXvUi2omocCFxGRc6GfzyKedGQnTB4CxtuRgIXVO6XKlL/+YebKGB66pBmXta7BwpiFTN08lQtDLuSCuhcUf8wiIlIk1BIm4inH9sKkwZCVBrfPhSpNTqmyfNcRxs7exMUtq/PwJc04mHKQZxY/Q4tKLRgckvtWrCIiUpoUqCXMGDPSGFPROHxqjFlljLnc3cGJlFlJBx0JWMoRuPU7qNH6lCr7j6Vx39RV1K0UyJs3dABjGb14NKlZqbx6wav4Gt/ij1tERIpMQbsj77TWHgMuByoBtwGvuC0qkbIsNR6mXAXxe+CWGVCn0ylVMrJyuHfKSpLTs/hoaBdCA335cuOX/LXvL57o9gSNwxoXf9wiIlKkCpqEGef//YHJ1tqNLtNEpKDSk2DqtXBwK9w4xXFB1jyMnb2RVbvjee3a9jSvEcLGQxuZuGoil9a/lGubXVvMQYuIiDsUNAlbaYz5CUcSNt8YEwLkuC8skTIoMw2+vgliV8G1nzluSZSH6ct3M3Xpbu65sDEDwmuRnJnMEwufoEpgFZ7v+TzG6PePiEhZUNCB+XcBHYAd1toUY0wV4A63RSVS1mRnwjfDYOdCuOojaDUwz2pr9sTzbMRGejeryhNXtARg3NJxxCTF8OnlnxLqH1qcUYuIiBsVtCVsMLDdWhvvfJ0NaFCKSEHkZMN3w+HveTDgf9D+hjyrHUxMZ8TklVSv6M/EGzvi7WWYu2Mus7bPYnj4cLrU7FLMgYuIiDsVNAl7zlqbcPyFMxl7zi0RiZQlOTkweyRs/A4uewG63p1ntczsHO7/ahVHUzL44NbOVAr2IyYxhv/76//oWL0j94TfU8yBi4iIuxW0OzKvZE3XGBM5HWth/mhYPRkueAJ6jcy36ri5m1m28whv3tCetnVCyczJ5MlFT2IwvNL7FXy89HETESlrCtoStsIY84Yxponz8Qaw0p2BiZR6v70ES9+HHvfBRaPzrfb96hg+/2MXd/RqyFUd6wLw/pr3WXdwHWN6jqF2hdrFFbGIiBSjgiZhDwIZwHTnIx24311BiZR6i9+Cha9Bp6FwxTjI54zGDbEJPPXdero3qszo/q0AWB63nE/Wf8JVTa+ib8O+xRi0iIgUpwL1cVhrk4FRbo5FpGxY9jH88hy0vRaufCvfBOxocgYjpqykUpAf79zcCV9vL+LT4hm1aBQNKjZgVDd95EREyrLTJmHGmLestQ8bY2YDNne5tXaQ2yITKY3WTIO5j0GL/nDVB+DlnWe17BzLQ1+v5sCxdGaMOI9qIf5Ya3luyXMcSTvC2/3fJsg3qJiDFxGR4nSmlrDJzv9fd3cgIqXepkiIvA8aXQjXfg7e+d/b8bX5W1m07RDjr2lHh3phAHzz9zf8uudXHuvyGK2rnHovSRERKVtOm4RZa1caY7yB4dbaW4opJpHSZ9svMPMuqNsVbvwKfAPyrTpn3T4++H07N3evzw1d6wMQfTSaV5e/Sq/avbit9W3FFbWIiHjQGQfmW2uzgQbGGL9iiEek9Nn1B0y/Baq3gptngH+FfKtujUvk8Zlr6VQ/jOcGOlq70rPTeWLREwT7BvPi+S/iZQp6voyIiJRmBb340A7gD2PMLCD5+ERr7RtuiUqktIhdCV/dAGEN4LbvITAs36oJqZncM3kFwf4+vH9rZ/x9HOPF/rfif2w7uo33L32fqoFViylwERHxtIImYdudDy8gxDntlIH6IuXK/o0w+WoIqgxDIyA4/wQqJ8fyyPQ1xBxNZdrwHtSo6OiujNoTxbQt07it9W2cX+f84olbRERKhIImYZustd+4TjDGXOeGeERKh0PRMGkI+AbBsFlQ8fQXVH1rwTZ+3XKA/xvchq4NKwNwIOUAz/7xLK0qt+LhTg+7P2YRESlRCjr45KkCThMp++J3w6TBYHNgaCRUanja6j9v2s/EBdu4tnNdbu3RAIAcm8PoxaNJz05n/AXj8fPWkEsRkfLmTNcJ6wf0B+oYYya6FFUEstwZmEiJlBjnSMDSE+H2H6Ba89NW334wiUenr6FdnVBeHNIW47xw6+cbPmfpvqWM7TmWRqGNiiNyEREpYc7UHbkXWAEM4uR7RSYCj7grKJESKeUITL4KEvc7xoDVCj9t9aT0LO6ZvBJfHy8+uK0zAb6OgfjrD67nndXvcHmDy7mq6VXFELiIiJREZ7pO2FpgrTHmK2fd+tbarcUSmUhJknYMplwNh7fDLd9AvW6nrW6t5bEZa9l5KJnJd3WjTlggAMmZyTy56EmqBVXjuZ7PnWgZExGR8qegY8L6AmuAeQDGmA7Oy1WIlH0ZKY7LUMSth+snQeMLzzjLe1Hbmbcxjqf6taRnk3/Pmnzpr5eITYpl/AXjqehX0Z1Ri4hICVfQJOx5oBsQD2CtXQNoIIuUfVnpMP1W2PMXXP0RtOh7xlmith7g9Z+2MrB9be46/9+PyQ87fmD2jtmMCB9Bx+od3Rm1iIiUAgVNwjKttQm5puk6YVK2ZWfBzDth+wIYOBHaXnPGWXYfTmHk12toUSOE8de0O9HduCdxDy/+9SKdqnfiP+H/cXfkIiJSChT0OmEbjTE3A97GmGbAQ8AS94Ul4mE5ORB5P2z5AfqOh05nvp9jSkYWwyevAOCj27oQ5Of4eGXmZPLkwifxMl680vsVfLwK+rETEZGyrKAtYQ8CbYB0YBpwDHjYTTGJeJa1MPe/sO5ruPgZ6DGiALNYRn27nq37E5lwYwfqVwk6UfbemvdYf2g9z5/3PLUq1HJn5CIiUooU6Ce5tTYFeNr5ECm7rIWfx8CKz6DXSOj9WIFm+3TxTmat3cvjV7SgT4vqJ6Yv3beUT9d/yjXNruHyhpe7K2oRESmFznSx1tOeAWmtHVS04Yh42MLXYclE6Ho3XDoWCnAJiSXbD/Hyj1vo26Ym9/VpcmL60bSjjF40moahDXmi6xPujFpEREqhM7WEnQfswdEFuRTQRY2k7PrzPfjtRWh/E/R7rUAJWGx8Kg98tZpGVYN5/fr2JwbiW2sZs2QMR9OP8u6l7xLkG3SGJYmISHlzpiSsJnAZcBNwMzAHmGat3ejuwESK1apJMP8paDUIBr0DXmceLpmWmc2IySvJzMrhw9s6U8H/34/T9K3TidoTxRNdn6Bl5ZZuDFxEREqr0/6lsdZmW2vnWWuHAT2AaCDKGPNAsUQnUhzWz4RZD0HTS+GaT8H7zEMlrbU8E7GB9bEJvHFDB5pUq3Ci7O+jf/Pa8tc4v8753NrqVndGLiIipdgZ/9oYY/yBAThawxoCE4Hv3RuWSDHZ+iN8fw806AnXTwYfvwLNNuWvf5i5MoaHLmnGZa1rnJielpXGkwufJMQvhBd7vajbEomISL7ONDB/EtAWmAuMtdZuKJaoRIrDjiiYMQxqhsNNX4NfwcZtLd91hLGzN3Fxy+o8fEmzk8peX/E60fHRfHjph1QJrOKGoEVEpKw4U0vYrUAyMBJ4yOVXvQGstVY3v5PSafdSmHYTVGkCt34LAQV7K+8/lsZ9U1dRt1Igb97QAS+vf1u6FuxewPSt0xnWehg96/R0V+QiIlJGnDYJs9YW9GKuIqXHvrUw9ToIqQW3RUBQ5QLNlpGVw71TVpKcnsXUu7sTGuh7oiwuOY7nljxHq8qtGNlppJsCFxGRskT3T5Hy5eBWmHyVo+VraCSE1DjzPE5jZ29k1e543r25E81rhJyYnp2TzejFo8nIzuDVC17F19v3NEsRERFxUEuXlB9HdsKkwWC8HQlYWL0Czzp9+W6mLt3NiAubMCD85FsPfbbhM5bHLeepbk/RMLRhEQctIiJllVrCpHw4tteRgGWlwe1zHWPBCmjNnniejdhI72ZVefyKFieVrT24lnfXvEvfhn0Z0nRIEQctIiJlmZIwKfuSDjoSsJQjMGwW1Ghd4FkPJqYzYvJKqlf0Z+KNHfF2GYifmJHIkwufpEZQDZ4971ldjkJERApFSZiUbanxMOUqiN8Dt30HdToVeNbM7Bzu/2oV8akZfHtvTyoF/3sNMWstL/71InHJcXzR9wsq+ulEYRERKRyNCZOyKz0Jpl4LB7bAjVMcF2QthHFzN7Ns5xFeuTqcNrVDTyqbvWM2c3fOZUT7EXSo3qEIgxYRkfJCLWFSNmWmwdc3QewquO4Lxy2JCuH71TF8/scu7ujVkCEd65xUtvvYbl766yU61+jMf9r9pwiDFhGR8kRJmJQ92Znwze2wcyFc9SG0HlSo2TfEJvDUd+vp3qgyo/u3OqksMzuTJxY+gY+XD6/0fgVvL+8iDFxERMoTJWFStuRkw3fD4e8fYcD/oP2NhZr9aHIGI6aspFKQH+/c3Alf75N77N9e8zYbD2/kzT5vUjO4ZlFGLiIi5YySMCk7cnJg9kjY+B1c9gJ0vbtQs2fnWB76ejUHjqUzY8R5VAvxP6n8z71/8vmGz7m2+bVc2qBw3ZsiIiK5KQmTssFamD8aVk+GC56AXoW/ddBr87eyaNshxl/Tjg71wk4qO5J2hNGLR9M4tDFPdH2iiIIWEZHyzK1nRxpj+hpjthpjoo0xo05T7xpjjDXGdHFnPFKG/fYSLH0fetwHF40u9Oxz1u3jg9+3c0v3+tzQtf5JZdZanv3jWRLSE3j1glcJ9AksqqhFRKQcc1sSZozxBt4F+gGtgZuMMadcJdMYEwKMBJa6KxYp4xa/BQtfg463wRXjoJAXTd0al8jjM9fSqX4Yzw1sc0r5V1u+YmHMQv7b5b+0qNwijyWIiIgUnjtbwroB0dbaHdbaDOBrYHAe9f4PGA+kuTEWKauWfQy/PAdtr4GBEwqdgCWkZnLP5BUE+/vw/q2d8fM5+SOx9chW3ljxBhfUvYCbW95clJGLiEg5584xYXWAPS6vY4DurhWMMZ2AetbaOcaYx/NbkDFmODAcoEaNGkRFRRV9tOVMUlJSqd+PNeJ+pdWWCRyq0pWNlW/GLlxUqPlzrGXCqnT2HMlmVLcANq/6i80u5Rk5GbwW9xoBJoC+9OX3338v2g04R2XhGJZnOn6ln45h6efpY+ixgfnGGC/gDeD2M9W11n4EfATQpUsX26dPH7fGVh5ERUVRqvfjplnw+9vQ6EKq3jyDC30DCr2IN37+m7UHt/F/g9tw23kNTyl/4c8XiMuM48PLPqRn7cJdbb84lPpjWM7p+JV+Ooaln6ePoTu7I2OBei6v6zqnHRcCtAWijDG7gB7ALA3OlzPa9gvMvBPqdoUbv4KzSMB+3rSfiQu2cW3nutzao8Ep5b/88wvf/P0Nd7S5o0QmYCIiUvq5MwlbDjQzxjQyxvgBNwKzjhdaaxOstVWttQ2ttQ2Bv4BB1toVboxJSrtdf8D0W6B6K7h5BvhXKPQith9M4tHpa2hXJ5QXh7TF5BpHFpccx3NLnqNNlTY82PHBoopcRETkJG5Lwqy1WcADwHxgMzDDWrvRGPOCMaZw95ERAYhdCV/dAGEN4LbvITCs0ItISs/inskr8fXx4oPbOhPge/Jth7Jzsnlq0VNk5mQy/oLx+Hr7FlHwIiIiJ3PrmDBr7Vxgbq5pY/Kp28edsUgpt38jTL4agirD0AgIrlroRVhreWzGWnYeSmbyXd2oE3bq9b4+Wf8JK/av4MVeL9Kg4qndlCIiIkXFrRdrFSkSh6Jh0hDwDYRhs6Bi7bNazHtR25m3MY6n+rWkZ5NTk7g1B9bw/tr36deoH4OaqLFWRETcS7ctkpItfg9MGgw2G4b+AJUantViorYe4PWftjKofW3uOr/RKeWJGYmMWjSKmsE1ebbHs6eMExMRESlqSsKk5ErcD5MGQXoi3D4bqp3d1ep3H05h5NdraFEjhPHXhJ+SYFlr+b8//4+45Di+7PclIX4hRRG9iIjIaak7UkqmlCMweYgjEbt1JtRqf3aLychi+GTHCbcf3daFQD/vU+pEbo/kx10/cl+H+2hf7ezWIyIiUlhqCZOSJ+0YTLkaDm+HW76Bet3OajHWWkZ9u56t+xP54o5u1K8SdEqdXQm7GLd0HF1rduWutneda+QiIiIFppYwKVkyUhyXoYhbD9dPgsYXnvWiPl28k1lr9/LY5S24sHm1U8ozszN5ctGT+Hn78fL5L+PtdWormYiIiLuoJUxKjqx0mH4r7P4Trv0UWvQ960Ut2X6Il3/cQt82NbmvT5M860xcPZFNhzcx4aIJ1AiucdbrEhERORtKwqRkyM5y3Ipo+wIY9Da0veasFxUbn8oDX62mUdVgXr++fZ5nOi6JXcIXG7/ghhY3cHH9i88lchERkbOi7kjxvJwciLwftvwAfV+BTkPPelFpmdmMmLySzKwcPrytMxX8T/2dcTj1MKMXj6ZpWFMe6/LYuUQuIiJy1tQSJp5lLcx9DNZ9DRc9Az3uPYdFWZ6J2MD62AQ+HtqFJtVOva+ktZZn/3iWxIxEPrr8IwJ8Cn/zbxERkaKgljDxHGvhl+dgxafQayRccG6tUlP++oeZK2N46JJmXNY67zFeUzdPZVHsIh7r+hjNKzU/p/WJiIicCyVh4jkLX4c/JkDXu+HSsXAOV6lfvusIY2dv4pKW1Xn4kmZ51tlyZAtvrHyDPvX6cGOLG896XSIiIkVBSZh4xp/vwW8vQvuboN9r55SA7T+Wxn1TV1GvchBv3NABL69Tl5WSmcITC5+gkn8lXuj5gm5LJCIiHqcxYVL8Vk2C+U9Bq4Ew6B3wOvvfAhlZOdw7ZSXJ6VlMvbs7oYG+edZ7dfmr7ErYxceXf0ylgEpnvT4REZGiopYwKV7rZ8Ksh6DppXDNp+B9br8Dxs7eyKrd8bx+XXua18j7no8/7fqJb7d9y51t76R7re7ntD4REZGiopYwV+tmwIIXICEGQuvCJWMg/HpPR1W6ue7ToCqQchga9ITrJ4OP/zktevry3UxdupsRFzahf7taedbZl7SP5/98nnZV23F/x/vPaX0iIiJFSUnYcetmwOyHIDPV8Tphj+M1KBE7W7n3acohx9iv8BvA79T7OBbGmj3xPBuxkd7NqvL4FS3yrJOdk82oRaPIsTmM7z0eX6+8uypFREQ8QUnYcQte+DdZOC4zFeaNOucWm5Ko6sGNsCnBvSuZN+rUfWotLHwNOg8768UeTExnxOSVVK/oz8QbO+Kdx0B8gI/Wf8SqA6sYd/446lWsd9brExERcQclYcclxOQ9PeUwzDj7K7iXVG0BNnpo5fnt6wLIzM7h/q9WEZ+awbf39qRSsF+e9VYfWM0Haz/gysZXMrDJwLNen4iIiLsoCTsutK6jCzK3CjXgtu+LPx43W758OV27dnXvSiZfBUn7T50eWvesFzlu7maW7TzCWzd0oE3t0DzrHMs4xpMLn6R2cG2e7v70Wa9LRETEnZSEHXfJmJPHLwH4BsLlL0KNNp6Ly02SKxx0/3Zd/mLe+/SSMWe1uO9Xx/D5H7u4o1dDhnSsk2cday0v/PkCB1MOMqnfJCr4nXrrIhERkZJAl6g4Lvx6GDgRQusBxvH/wIkalH8uinCfbohN4Knv1tO9UWVG92+Vb72I6Ajm75rP/R3vp121ducQvIiIiHupJcxV+PVKuopaEezTo8kZjJiykkpBfrx7Syd8vfP+7bAzYScvL3uZ7jW7c2fbO89pnSIiIu6mJExKtOwcy0Nfr+bAsXRmjDiPqhXyPlM1IzuDJxc+ib+3P+N6j8PLqJFXRERKNiVhUqK9Nn8ri7Yd4tVrwulQLyzfehNWTWDzkc28ffHbVA+qXnwBioiInCU1F0iJNWfdPj74fTu3dK/P9V3zv87X4tjFTNo0iRtb3Eifen2KL0AREZFzoCRMSqStcYk8PnMtneqH8dzA/M/iPJR6iKcXP03TsKb8t8t/izFCERGRc6PuSClxElIzuWfyCoL9fXj/1s74+eT9WyHH5vDMH8+QnJnMp5d/SoBPQDFHKiIicvbUEiYlSk6O5ZHpa4g5msr7t3SiRsX8E6spm6bwR+wfPN7lcZpWalqMUYqIiJw7JWFSory1YBu/bjnAcwNb06Vh5XzrbT68mTdXvcnF9S7m+ha6rIiIiJQ+SsKkxPh5034mLtjGtZ3rcmuPBvnWS8lM4YmFT1A5oDJje47FmLxv4C0iIlKSaUyYlAjbDybx6PQ1hNcN5cUhbU+bWI1fPp5/jv3DJ5d/QlhAWPEFKSIiUoTUEiYel5SexT2TV+Lr48X7t3YmwNc737rzd83nu23fcXe7u+lWq1sxRikiIlK01BImHmWt5bEZa9l5KJnJd3WjTlhgvnX3Ju1l7JKxhFcN594O9xZjlCIiIkVPLWHiUe9FbWfexjie6teSnk2q5lsvKyeLJxc+SQ45vHLBK/h6+RZjlCIiIkVPLWHiMVFbD/D6T1sZ3KE2d53f6LR1P1z3IWsOruGV3q9QLyT/q+eLiIiUFmoJE4/YfTiFkV+voUWNEF65Ovy0A/FX7l/JR+s+YlCTQQxoPKAYoxQREXEfJWFS7FIyshg+eQUAH93WhUC//AfiJ6QnMGrRKOpWqMvo7qOLK0QRERG3U3ekFCtrLaO+Xc/W/Yl8cUc36lcJOm3dsX+O5VDKISb3n0ywb3AxRioiIuJeagmTYvXp4p3MWruXxy5vwYXNq5227rfbvuXnf37mwU4P0rZq22KKUEREpHgoCZNis2T7IV7+cQt929Tkvj5NTlt3R/wOxi8bT49aPbi9ze3FE6CIiEgxUnekuFXE6lhem7+VvfGpGANVK/jz+vXtTzsQPz07nScWPkGgTyDjzh+Hl9FvBRERKXv0103cJmJ1LE99t57Y+FQskGMhITWTXzbtP+18b618i61Ht/J/vf6PakGn77IUEREprZSEidu8Nn8rqZnZJ01Lz8rhtflb851nYcxCpmyews0tb+bCehe6O0QRERGPURImbrM3PrVQ0w+mHOTZP56leaXmPNrlUXeGJiIi4nFKwqTIWWuZvnw3Np/y2nncHzLH5vD04qdJyUzh1Qtexd/b371BioiIeJgG5kuRSk7P4pmIDXy/OpZm1YPZcySVtKycE+WBvt48fkWLU+abtHESf+77k2d7PEuTsNOfOSkiIlIWKAmTIrMl7hj3TV3FrkPJPHpZc+6/qCmz1+49cXZk7bBAHr+iBUM61jlpvo2HNzJh9QQurX8p1zW/zkPRi4iIFC8lYXLOHN2Pe3hu1kYqBvoy5e7u9GxSFYAhHeucknS5SslM4cmFT1IloArP93z+tJeuEBERKUuUhMk5SUrP4unv1xO5Zi/nN63Kmzd0oFpIwcdzjVs6jt3HdvPpFZ8S6h/qxkhFRERKFiVhctY27T3GA1+tYtfhZP57WXPuu6gp3l4Fb8n6ceePRG6PZHj4cLrW7OrGSEVEREoeJWFSaNZapi3bw/OzNxIW6MtX/+lBj8ZVCrWMmMQYXvjzBdpXa8+97e91U6QiIiIll1svUWGM6WuM2WqMiTbGjMqj/FFjzCZjzDpjzAJjTAN3xiPnLjEtk4e+XsPo79fTvVFl5o7sXegELCsni1GLHG+H8ReMx8dLvwVERKT8cdtfP2OMN/AucBkQAyw3xsyy1m5yqbYa6GKtTTHG3Au8Ctzgrpjk3Gzcm8ADX63mn8PJPH5FC+69sAleheh+PO79te+z9uBaXr3gVepUyH/QvoiISFnmziaIbkC0tXYHgDHma2AwcCIJs9b+5lL/L+BWN8YjZ8lay9Slu3nhh01UCvJl2n960L2QrV/HLY9bzsfrPmZwk8H0a9SviCMVEREpPdyZhNUB9ri8jgG6n6b+XcCPboxHzkJiWiZPfbeeH9bt44Lm1Xjz+vZUqXB2V7NPSE/gqUVPUb9ifUZ3H13EkYqIiJQuJWIwjjHmVqALkOcdm40xw4HhADVq1CAqKqr4giujkpKSzrgf/zmWzXtr0jmYarm2uS/9GyWzfsWfZ7U+ay2fHPyEQ6mHeLTmoyz7Y9lZLUf+VZBjKCWXjl/pp2NY+nn6GLozCYsF6rm8ruucdhJjzKXA08CF1tr0vBZkrf0I+AigS5cutk+fPkUebHkTFRVFfvvRWsuUv/7hpaWbqRzsz/R7OtK1YeWzWs+cHXOYsGoC+5L3AdC/YX+GXjj0bMMWF6c7hlLy6fiVfjqGpZ+nj6E7z45cDjQzxjQyxvgBNwKzXCsYYzoCHwKDrLUH3BiLFNCxtEzu/2oVz0ZupGfTKswd2fucErDnlzx/IgED+HXPr8zZMaeowhURESm13JaEWWuzgAeA+cBmYIa1dqMx5gVjzCBntdeACsA3xpg1xphZ+SxOisH6mASunLiY+Rv3M6pfSz4b1pXKwX5nvbwJqyaQlp120rS07DQmrJpwrqGKiIiUem4dE2atnQvMzTVtjMvzS925fikYay2T/vyHl+ZspkoFP6YP70GXs2z9Om5F3IqTWsBcxSXHndOyRUREyoISMTBfPOdYWiZPzlzHjxviuLhldf53XXsqnUPrV0xiDG+sfIOf//kZL+NFjs05pU7N4JrnErKIiEiZoCSsHFsXE88DX60mNj6V0f1bcvf5jc/q4qsAyZnJfLzuYyZtmoSPlw/3d7ifGkE1GLd03EldkgHeAYzsNLKoNkFERKTUUhJWDllr+XlXJjN+XkK1Cv7MuKcHnRucXfdjjs0hMjqSCasmcDjtMIOaDOKhjg9RI7gGAH7efkxYNYG45DhqBtdkZKeRDGg8oCg3R0REpFRSElbOJKRm8sTMtczfksGlrarz2rVn3/24Im4Fry5/lc1HNtO+Wnvevvht2lVrd1KdAY0HKOkSERHJg5KwcmTNnnge+GoVcQlp3NjCj5eHdsGYwnc/uo77qhlck/G9x9OvUb+zWpaIiEh5pSSsHLDW8vkfu3j5x81UDwlgxojzOLZjbaGTpuTMZD5Z/wmTNk7C28ub+zvcz7A2wwj0CXRT5CIiImWXkrAyLiElk8dnruWnTfu5tFUNXr8unLAgP6J2FHwZx8d9TVw9kUOphxjYeCAPdXpIZzmKiIicAyVhZdiaPfHcP3UV+4+l8cyAVtx1fqNCt36t3L+S8cvGnxj3NfGiiaeM+xIREZHCUxJWBllr+XTxTl75cQs1KgbwzYjz6Fi/UqGWEZMYw5sr3+Snf36iRlANjfsSEREpYkrCypj4lAwe+2Ydv2zez+Wta/Date0JDfIt8Py5x33d1+E+bm9zu8Z9iYiIFDElYWXIqt1HefCr1RxITOO5ga25vWfDArdc5R73dWXjKxnZaaTGfYmIiLiJkrAywFrLJ4t2Mn7eFmqGBjBzRE/a1wsr8Pyu477Cq4Uz4aIJhFcLd1/AIiIioiSstDuanMFj36xlwZYD9G1Tk/HXhhMaWLDux9ikWN5Y8caJcV+v9H6F/o36a9yXiIhIMVASVoqt/OcoD361ioNJ6Tw/sDXDCtj9mJKZwuyjs4n6Pgov48V97e/j9rYa9yUiIlKclISVQjk5lo8X7eC1+VupFRbAt/f2JLxu2JnnsznM2j6LCasmaNyXiIiIhykJK2WOJmfw32/W8uuWA/RrW5NXrilY9+Oq/asYv3w8mw5vIrxqOENDh3JH7zuKIWIRERHJi5KwUmTFriM8OG01h5MyeGFwG27r0eCM3Y+xSbG8ufJN5u+aT/Wg6rzc+2X6N+rPwt8XFlPUIiIikhclYaVATo7lw4U7eP2nrdStFMh39/WkbZ3Q086TkpnCJ+s/4cuNX54Y9zWszTCCfIOKKWoRERE5HSVhJdyR5AwenbGGqK0HGdCuFi9f046KAfl3P+bYHGZvn82EVRM4mHqQAY0H8HCnhzXuS0REpIRRElaCLd91hAe/Ws2R5Az+b0hbbu1e/7Tdj7nHfb150Zu0r9a+GCMWERGRglISVgLl5Fg+WLid//30N/UK0P24N2kvb6x845RxX17GqxijFhERkcJQElbCHE5K59EZa/n974NcGV6Ll69uR0g+3Y+5x33d2/5ebm9zu8Z9iYiIlAJKwkqQZTuP8OC0VRxNyeTFIW25JZ/uR437EhERKf2UhJUAOTmW96KieePnv2lQJZjPbu9Km9p5dz+uPrCa8cvGs/HwRtpVbadxXyIiIqWUkjAPO5SUziPT17Bo2yEGta/NuKvbUcH/1MOyN2kvb658k3m75lE9qDrjzh/HgMYDNO5LRESklFIS5kF/7TjMQ9NWE5+ayctXt+PGrvVO6X5MyUzh0w2f8uXGLzEYjfsSEREpI5SEeUB2juW936J585e/aVglmC/u6Ebr2hVPqpNjc/hhxw9MWDmBA6kH6N+oP490fkTjvkRERMoIJWHF7GCio/txcfQhBneozUtXndr9mHvc1//6/I8O1Tt4JmARERFxCyVhxWjJ9kOM/HoNx1IzeeXqdtyQq/txb9Je3lr5Fj/u+lHjvkRERMo4JWHFIDvH8s6v0UxY8DeNqgYz+a5utKz5b/ej67gvgBHtR3BHmzs07ktERKQMUxLmZgcS03hk+hr+iD7MVR3r8OKQtgQ7ux/zGvf1cKeHqVWhloejFhEREXdTEuZGS6IP8dDXa0hKz+TVa8K5rkvdE92Paw6sYfyy8Ww4vIG2Vdpq3JeIiEg5oyTMDbJzLBMXbGPir9toXDWYqXd3p0XNEAD2Je3jzZVvOsZ9BWrcl4iISHmlJKyIHUhMY+S0Nfy54zBXd6rD/w12dD+mZKbw2YbP+GLjF4DGfYmIiJR3SsKK0OJth3h4+mqS0rN47dpwrutS78R9Ht9a+RYHUg/Qr1E/Hun0iMZ9iYiIlHNKwopAdo5lwoJtvP3rNppWq8BX/+lB8xohGvclIiIi+VISdo4OHEvjoa9X89eOI1zXuS5jB7chIeMgTyz8P37cqXFfIiIikjclYedg0baDPDJ9Dcnp2bx+XXv6h1fm840f8sWGL7BY7gm/hzvb3qlxXyIiInIKJWFnISs7h7d+2ca7UdE0q16BqXd34O/khQz83jnuq2E/HumscV8iIiKSPyVhhbT/WBoPTlvNsp1HuL5LXa7pmc3Ylfey/tB62lRpo3FfIiIiUiBKwgrh978P8uj0NaRmZvPckNpszpjK3T//SLXAarx0/ktc2fhKjfsSERGRAlESVgBZ2Tm8+cvfvPvbdprV8OOCbpt5L/pZLJbh4cO5q+1dGvclIiIihaIk7AziEtJ4aNpqlu06xPkd9rDP61tmRDvGfT3c+WFqV6jt6RBFRESkFFIS5iJidSyvzd/K3vhUaocFMiC8JjNXxpLmtYNWnX9hbcoW2lRpw+t9Xqdj9Y6eDldERERKMSVhThGrYxn905eYKj8SXDOe+MwwPl/XmwpVY/EOWkW6rcaLvV5kYJOBGvclIiIi50xJmNNLv0/Fq/pMjFcmAMYvHv+as8nCS+O+REREpMgpCXNKCZ6NlzMBO84YyMmswIMdH/RQVCIiIlJWqV/Nycs3Pu/pPseKNxAREREpF5SEOYX6VS/UdBEREZFzoSTM6akej+Jr/E+a5mv8earHox6KSERERMoyjQlzGtB4AAATVk0gLjmOmsE1Gdlp5InpIiIiIkVJSZiLAY0HKOkSERGRYqHuSBEREREPcGsSZozpa4zZaoyJNsaMyqPc3xgz3Vm+1BjT0J3xiIiIiJQUbkvCjDHewLtAP6A1cJMxpnWuancBR621TYE3gfHuikdERESkJHFnS1g3INpau8NamwF8DQzOVWcw8KXz+UzgEmOMcWNMIiIiIiWCOwfm1wH2uLyOAbrnV8dam2WMSQCqAIdcKxljhgPDAWrUqEFUVJSbQi4/kpKStB9LOR3D0k3Hr/TTMSz9PH0MS8XZkdbaj4CPALp06WL79Onj2YDKgKioKLQfSzcdw9JNx6/00zEs/Tx9DN3ZHRkL1HN5Xdc5Lc86xhgfIBQ47MaYREREREoEdyZhy4FmxphGxhg/4EZgVq46s4BhzufXAr9aa60bYxIREREpEdzWHekc4/UAMB/wBj6z1m40xrwArLDWzgI+BSYbY6KBIzgSNREREZEyz61jwqy1c4G5uaaNcXmeBlznzhhERERESiJT2nr/jDEHgX88HUcZUJVcZ6FKqaNjWLrp+JV+OoalX3EcwwbW2mp5FZS6JEyKhjFmhbW2i6fjkLOnY1i66fiVfjqGpZ+nj6HuHSkiIiLiAUrCRERERDxASVj59ZGnA5BzpmNYuun4lX46hqWfR4+hxoSJiIiIeIBawkREREQ8QElYGWSMqWeM+c0Ys8kYs9EYM9I5vbIx5mdjzDbn/5Wc040xZqIxJtoYs84Y08mzWyDHGWO8jTGrjTE/OF83MsYsdR6r6c67UWCM8Xe+jnaWN/Ro4AKAMSbMGDPTGLPFGLPZGHOePoelhzHmEed36AZjzDRjTIA+gyWbMeYzY8wBY8wGl2mF/swZY4Y5628zxgzLa11FQUlY2ZQF/Nda2xroAdxvjGkNjAIWWGubAQucrwH6Ac2cj+HA+8UfsuRjJLDZ5fV44E1rbVPgKHCXc/pdwFHn9Ded9cTzJgDzrLUtgfY4jqU+h6WAMaYO8BDQxVrbFsedX25En8GS7gugb65phfrMGWMqA88B3YFuwHPHE7eipiSsDLLW7rPWrnI+T8TxxV8HGAx86az2JTDE+XwwMMk6/AWEGWNqFW/Ukpsxpi4wAPjE+doAFwMznVVyH8Pjx3YmcImzvniIMSYUuADH7dmw1mZYa+PR57A08QECjTE+QBCwD30GSzRr7UIct0F0VdjP3BXAz9baI9bao8DPnJrYFQklYWWcs0m8I7AUqGGt3ecsigNqOJ/XAfa4zBbjnCae9RbwBJDjfF0FiLfWZjlfux6nE8fQWZ7grC+e0wg4CHzu7FL+xBgTjD6HpYK1NhZ4HdiNI/lKAFaiz2BpVNjPXLF9FpWElWHGmArAt8DD1tpjrmXWcVqsTo0toYwxVwIHrLUrPR2LnDUfoBPwvrW2I5DMv90ggD6HJZmz+2kwjmS6NhCMm1pDpPiUtM+ckrAyyhjjiyMBm2qt/c45ef/x7g3n/wec02OBei6z13VOE8/pBQwyxuwCvsbRBTIBR3O5j7OO63E6cQyd5aHA4eIMWE4RA8RYa5c6X8/EkZTpc1g6XArstNYetNZmAt/h+FzqM1j6FPYzV2yfRSVhZZBzHMKnwGZr7RsuRbOA42d5DAMiXaYPdZ4p0gNIcGm6FQ+w1j5lra1rrW2IYzDwr9baW4DfgGud1XIfw+PH9lpn/RLza688stbGAXuMMS2cky4BNqHPYWmxG+hhjAlyfqceP376DJY+hf3MzQcuN8ZUcraIXu6cVuR0sdYyyBhzPrAIWM+/44lG4xgXNgOoD/wDXG+tPeL8gnkHR1N7CnCHtXZFsQcueTLG9AEes9ZeaYxpjKNlrDKwGrjVWptujAkAJuMY/3cEuNFau8NDIYuTMaYDjhMr/IAdwB04fvzqc1gKGGPGAjfgOON8NXA3jrFB+gyWUMaYaUAfoCqwH8dZjhEU8jNnjLkTx99NgJestZ+7JV4lYSIiIiLFT92RIiIiIh6gJExERETEA5SEiYiIiHiAkjARERERD1ASJiIiIuIBSsJEyjljjDXG/M/l9WPGmOeLaNlfGGOuPXPNc17PdcaYzcaY31ymtTPGrHE+jhhjdjqf/1LAZQ4yxow6Q53axpiZp6tTUMaY240x7xTFskSkdPA5cxURKePSgauNMS9baw95OpjjjDE+LvfoO5O7gP9Yaxcfn2CtXQ90cC7rC+AHa+1JCdPp1mGtnYXjYo75stbu5d8Ld4qIFIpawkQkC/gIeCR3Qe6WLGNMkvP/PsaY340xkcaYHcaYV4wxtxhjlhlj1htjmrgs5lJjzApjzN/Oe2JijPE2xrxmjFlujFlnjLnHZbmLjDGzcFydPHc8NzmXv8EYM945bQxwPvCpMea1M22sMSbKGPOWMWYFMNIYM9AYs9R5k+1fjDE1nPVOtEw598NEY8wS5/Ze65ze0BizwaX+d8aYecaYbcaYV13WeZdz+5cZYz4+U4uXc33vG2P+cq6vjzHmM2dr3xcu9d537tuNzguLHp/e3xizxRiz0hn3D87pwc7lLHNu72Dn9DbOaWucx6PZmfajiJw7tYSJCMC7wDrXxKEA2gOtcFwdfAfwibW2mzFmJPAg8LCzXkOgG9AE+M0Y0xQYiuMWIV2NMf7AH8aYn5z1OwFtrbU7XVdmjKkNjAc6A0eBn4wxQ6y1LxhjLsZxV4GCXmHez1rbxbncSkAPa601xtwNPAH8N495auFI9lriaCHLqxuyA44rpqcDW40xbwPZwLPO7UoEfgXWFiDGSsB5wCDn+nrhuGL7cmNMB2vtGuBp55W/vYEFxphw4G/gQ+ACa+1O47iC+HFP47idzp3GmDBgmbN7dgQwwVo71RjjB3gXID4ROUdKwkQEa+0xY8wk4CEgtYCzLT9+b0NjzHbgeBK1HrjIpd4Ma20OsM0YswNHEnM5EO7SyhYKNAMygGW5EzCnrkCUtfagc51TgQtw3JKksKa7PK8LTDeOG/v6AXmtGyDCuR2bjreW5WGBtTbBGd8moAGO26f8bq094pz+DdC8ADHOdiaG64H9zu5VjDEbcSS2a4DrjTHDcXyX1wJa4+jh2OGyD6cBw53PL8dxY/jHnK8DcNzK5U/gaWNMXeA7a+22AsQnIudI3ZEictxbOMZWBbtMy8L5PWGM8cKRpByX7vI8x+V1Dif/wMt9bzQLGOBBa20H56ORtfZ4Epd8LhtRQK7reBt4x1rbDrgHR2KSF9ftNQWok825/dB13Z+597WPMaYR8BhwibU2HJhD/rEfZ4BrXPZ7fWvtZmvtVzha3FKBuc6WRRFxMyVhIgKAs6VmBo5E7LhdOLr/wPFH2vcsFn2dMcbLOU6sMbAVmA/ca4zxBTDGNDfGBJ9uIcAy4EJjTFVn99tNwO9nEU9uoUCs8/mwIlhebstxxF3JGOMDXFNEy62II5lMcLbM9XNO3wo0NsY0dL6+wWWe+cCDxhgDYIzp6Py/MY7Ws4lAJBBeRDGKyGmoO1JEXP0PeMDl9cdApDFmLTCPs2ul2o0jgaoIjLDWphljPsHRpbbKmRAcBIacbiHW2n3GccmI33C06Myx1kaeRTy5PQ98Y4w5imO8VqMiWOYJ1tpYY8w4HPvgCLAFSCiC5a41xqx2Lm8P8Idzeqox5j5gnjEmGUcSeNz/4WjxXOds2dwJXAlcD9xmjMkE4oBx5xqfiJyZsTZ3T4GIiBQlY0wFa22SsyXse+Aza+33xbA+g+Oki23W2jfdtT4ROTvqjhQRcb/njTFrgA04Wp8i3Ly+/zjXtxFHd+uHbl6fiJwFtYSJiIiIeIBawkREREQ8QEmYiIiIiAcoCRMRERHxACVhIiIiIh6gJExERETEA5SEiYiIiHjA/wMK5u0c8E8Z0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_sizes = [100, 250, 500, 750, 1000]\n",
    "\n",
    "precision_list, recall_list, map50_list = [0.0323, 0.238, 0.815, 0.981, 0.982], [0.321,0.321, 0.804, 0.982, 0.952], [0.0298, 0.187, 0.821, 0.994, 0.991]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dataset_sizes, precision_list, label='Precision', marker='o')\n",
    "plt.plot(dataset_sizes, recall_list, label='Recall', marker='o')\n",
    "plt.plot(dataset_sizes, map50_list, label='mAP50', marker='o')\n",
    "plt.xlabel('Number of Training Images')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Model Performance vs. Number of Training Images')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/Users/bhavnasharma/Downloads/SinGAN/yolov5/hold.yaml, weights=['/Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.8.3 torch-1.8.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/bhavnasharma/Downloads/SinGAN/yolov5/data/holdout.cache... \u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          2          0          0          0          0\n",
      "Speed: 1.2ms pre-process, 148.3ms inference, 2.5ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp15\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights /Users/bhavnasharma/Downloads/SinGAN/yolov5/runs/train/exp3/weights/best.pt --data /Users/bhavnasharma/Downloads/SinGAN/yolov5/hold.yaml \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bhavnasharma/Downloads/SinGAN\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/bhavnasharma/Downloads/SinGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete: 40 training images, 10 test images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Results\"\n",
    "new_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN\"\n",
    "\n",
    "train_images_dir = os.path.join(new_dir, \"train/images\")\n",
    "train_labels_dir = os.path.join(new_dir, \"train/labels\")\n",
    "\n",
    "test_images_dir = os.path.join(new_dir, \"test/images\")\n",
    "test_labels_dir = os.path.join(new_dir, \"test/labels\")\n",
    "\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "# Create directories if they dont exist\n",
    "os.makedirs(train_images_dir, exist_ok=True)\n",
    "os.makedirs(train_labels_dir, exist_ok=True)\n",
    "os.makedirs(test_images_dir, exist_ok=True)\n",
    "os.makedirs(test_labels_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Shuffle and split dataset (80% train, 20% test)\n",
    "random.shuffle(images)\n",
    "train_size = int(len(images) * 0.8)\n",
    "train_images = images[:train_size]\n",
    "test_images = images[train_size:]\n",
    "\n",
    "# **Copy** images and labels instead of moving them\n",
    "for img in train_images:\n",
    "    shutil.copy(os.path.join(images_dir, img[\"file_name\"]), os.path.join(train_images_dir, img[\"file_name\"]))\n",
    "    label_file = img[\"file_name\"].replace(\".png\", \".txt\")\n",
    "    shutil.copy(os.path.join(labels_dir, label_file), os.path.join(train_labels_dir, label_file))\n",
    "\n",
    "for img in test_images:\n",
    "    shutil.copy(os.path.join(images_dir, img[\"file_name\"]), os.path.join(test_images_dir, img[\"file_name\"]))\n",
    "    label_file = img[\"file_name\"].replace(\".png\", \".txt\")\n",
    "    shutil.copy(os.path.join(labels_dir, label_file), os.path.join(test_labels_dir, label_file))\n",
    "\n",
    "print(f\"Dataset split complete: {len(train_images)} training images, {len(test_images)} test images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 50 images and 50 labels to /Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_images = \"/Users/bhavnasharma/Downloads/SinGAN/Results/images\"\n",
    "source_labels = \"/Users/bhavnasharma/Downloads/SinGAN/Results/labels\"\n",
    "destination = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3\"\n",
    "\n",
    "# Create destination folders\n",
    "dest_images = os.path.join(destination, \"images\")\n",
    "dest_labels = os.path.join(destination, \"labels\")\n",
    "\n",
    "os.makedirs(dest_images, exist_ok=True)\n",
    "os.makedirs(dest_labels, exist_ok=True)\n",
    "\n",
    "# Copy images and labels\n",
    "for file_name in os.listdir(source_images):\n",
    "    shutil.copy(os.path.join(source_images, file_name), os.path.join(dest_images, file_name))\n",
    "\n",
    "for file_name in os.listdir(source_labels):\n",
    "    shutil.copy(os.path.join(source_labels, file_name), os.path.join(dest_labels, file_name))\n",
    "\n",
    "print(f\"Copied {len(os.listdir(dest_images))} images and {len(os.listdir(dest_labels))} labels to {destination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved COCO annotations to /Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/dataset_coco.json\n",
      "Saved COCO annotations to /Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/dataset_coco.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "image_dirs = {\n",
    "    \"train\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/images\",\n",
    "    \"test\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/images\"\n",
    "}\n",
    "\n",
    "label_dirs = {\n",
    "    \"train\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/labels\",\n",
    "    \"test\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/labels\"\n",
    "}\n",
    "\n",
    "output_json_paths = {\n",
    "    \"train\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/dataset_coco.json\",\n",
    "    \"test\": \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/dataset_coco.json\"\n",
    "}\n",
    "\n",
    "# Function to convert YOLO annotations to COCO format\n",
    "def convert_yolo_to_coco(image_dir, label_dir, output_json):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": 1, \"name\": \"pedestrian\"}]\n",
    "    annotation_id = 0\n",
    "\n",
    "    for idx, image_file in enumerate(os.listdir(image_dir)):\n",
    "        if not image_file.endswith(\".png\"):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        label_path = os.path.join(label_dir, image_file.replace(\".png\", \".txt\"))\n",
    "\n",
    "        # Get image dimensions\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Add image metadata\n",
    "        images.append({\n",
    "            \"file_name\": image_file,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"id\": idx\n",
    "        })\n",
    "\n",
    "        # Read YOLO label file\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                class_id, x_center, y_center, box_w, box_h = map(float, line.strip().split())\n",
    "                \n",
    "                # Convert normalized YOLO format to absolute COCO format\n",
    "                abs_x = max(0, int((x_center - box_w / 2) * width))\n",
    "                abs_y = max(0, int((y_center - box_h / 2) * height))\n",
    "                abs_width = max(1, int(box_w * width))\n",
    "                abs_height = max(1, int(box_h * height))\n",
    "                \n",
    "                abs_xmax = abs_x + abs_width\n",
    "                abs_ymax = abs_y + abs_height\n",
    "\n",
    "                # Ensure bounding box does not exceed image dimensions\n",
    "                abs_x = min(abs_x, width - 1)\n",
    "                abs_y = min(abs_y, height - 1)\n",
    "                abs_width = min(abs_width, width - abs_x)\n",
    "                abs_height = min(abs_height, height - abs_y)\n",
    "\n",
    "                \n",
    "\n",
    "                annotations.append({\n",
    "                    \"image_id\": idx,\n",
    "                    \"category_id\": int(class_id+1),\n",
    "                    \"bbox\": [abs_x, abs_y, abs_xmax, abs_ymax],\n",
    "                    \"area\": abs_width * abs_height,\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"id\": annotation_id\n",
    "                })\n",
    "                annotation_id += 1\n",
    "\n",
    "    # Save to COCO JSON file\n",
    "    coco_format = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    print(f\"Saved COCO annotations to {output_json}\")\n",
    "\n",
    "# Run conversion for train and test sets\n",
    "for split in [\"train\", \"test\"]:\n",
    "    convert_yolo_to_coco(image_dirs[split], label_dirs[split], output_json_paths[split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved COCO annotations to /Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/dataset_coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "image_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/images\"\n",
    "label_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/labels\"\n",
    "output_json = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/dataset_coco.json\"\n",
    "\n",
    "# Function to convert YOLO annotations to COCO format\n",
    "def convert_yolo_to_coco(image_dir, label_dir, output_json):\n",
    "    images, annotations = [], []\n",
    "    categories = [{\"id\": 1, \"name\": \"pedestrian\"}]  # Adjust categories if needed\n",
    "    annotation_id = 0\n",
    "\n",
    "    for idx, image_file in enumerate(os.listdir(image_dir)):\n",
    "        if not image_file.endswith(\".png\"):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        label_path = os.path.join(label_dir, image_file.replace(\".png\", \".txt\"))\n",
    "\n",
    "        # Get image dimensions\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Add image metadata\n",
    "        images.append({\"file_name\": image_file, \"width\": width, \"height\": height, \"id\": idx})\n",
    "\n",
    "        # Read YOLO label file\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                class_id, x_center, y_center, box_w, box_h = map(float, line.strip().split())\n",
    "\n",
    "                # Convert YOLO format to absolute COCO format\n",
    "                abs_x = int((x_center - box_w / 2) * width)\n",
    "                abs_y = int((y_center - box_h / 2) * height)\n",
    "                abs_width = int(box_w * width)\n",
    "                abs_height = int(box_h * height)\n",
    "                \n",
    "                abs_xmax = abs_x + abs_width\n",
    "                abs_ymax = abs_y + abs_height\n",
    "\n",
    "                # Ensure bounding box does not exceed image dimensions\n",
    "                abs_x = max(0, min(abs_x, width - 1))\n",
    "                abs_y = max(0, min(abs_y, height - 1))\n",
    "                abs_xmax = min(abs_xmax, width)\n",
    "                abs_ymax = min(abs_ymax, height)\n",
    "\n",
    "                annotations.append({\n",
    "                    \"image_id\": idx,\n",
    "                    \"category_id\": int(class_id + 1),  # Adjust class ID if needed\n",
    "                    \"bbox\": [abs_x, abs_y, abs_xmax, abs_ymax],\n",
    "                    \"area\": abs_width * abs_height,\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"id\": annotation_id\n",
    "                })\n",
    "                annotation_id += 1\n",
    "\n",
    "    # Save to COCO JSON file\n",
    "    coco_format = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    print(f\"Saved COCO annotations to {output_json}\")\n",
    "\n",
    "convert_yolo_to_coco(image_dir, label_dir, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Splitting done: Generated datasets for all training percentages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Load full dataset\n",
    "img_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/images\"\n",
    "ann_file = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN-3/dataset_coco.json\"\n",
    "full_dataset = CocoDetection(root=img_dir, annFile=ann_file, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Split percentages\n",
    "split_percentages = [0.02, 0.2, 0.4, 0.6, 0.8]  # First 1 image, then % of dataset\n",
    "num_images = len(full_dataset)\n",
    "\n",
    "# Generate training/testing splits\n",
    "split_results = []\n",
    "for split in split_percentages:\n",
    "    train_size = max(1, int(num_images * split))  # Ensure at least 1 image for training\n",
    "    indices = list(range(num_images))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    train_subset = Subset(full_dataset, train_indices)\n",
    "    test_subset = Subset(full_dataset, test_indices)\n",
    "\n",
    "    split_results.append((train_subset, test_subset))\n",
    "\n",
    "print(\"Splitting done: Generated datasets for all training percentages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    formatted_images, formatted_targets = [], []\n",
    "\n",
    "    image_annotations = defaultdict(list)\n",
    "    for anns in targets:\n",
    "        for ann in anns:\n",
    "            image_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    for img, anns in zip(images, targets):\n",
    "        image_id = anns[0][\"image_id\"] if anns else -1  \n",
    "        matched_annotations = image_annotations.get(image_id, [])\n",
    "\n",
    "        if matched_annotations:\n",
    "            valid_boxes = torch.tensor([ann[\"bbox\"] for ann in matched_annotations], dtype=torch.float32)\n",
    "            valid_labels = torch.tensor([ann[\"category_id\"] for ann in matched_annotations], dtype=torch.int64)\n",
    "        else:\n",
    "            valid_boxes = torch.empty((0, 4), dtype=torch.float32)  \n",
    "            valid_labels = torch.empty((0,), dtype=torch.int64)  \n",
    "\n",
    "        formatted_target = {\n",
    "            \"boxes\": valid_boxes,\n",
    "            \"labels\": valid_labels\n",
    "        }\n",
    "\n",
    "        formatted_images.append(img)\n",
    "        formatted_targets.append(formatted_target)\n",
    "\n",
    "    return formatted_images, formatted_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_subset, test_subset):\n",
    "    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_subset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "\n",
    "    num_epochs = 10\n",
    "    loss_values = []  # Track loss per epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for images, targets in train_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "\n",
    "            if len(targets) == 0:\n",
    "                continue\n",
    "\n",
    "            formatted_targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
    "            loss_dict = model(images, formatted_targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "        loss_values.append(total_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    all_precisions, all_recalls, all_f1s, all_conf = [], [], [], []\n",
    "\n",
    "    for i, (image, target) in enumerate(test_loader):\n",
    "        if not image or len(image) == 0:  # Prevent empty batch errors\n",
    "            print(f\" Skipping test image {i} due to empty batch\")\n",
    "            continue\n",
    "        \n",
    "        image_tensor = image[0].to(device)\n",
    "        \n",
    "        with torch.no_grad():  # No gradients for inference\n",
    "            output = model([image_tensor])\n",
    "\n",
    "        pred_boxes = output[0][\"boxes\"].cpu()\n",
    "        pred_labels = output[0][\"labels\"].cpu()\n",
    "        pred_scores = output[0][\"scores\"].cpu()\n",
    "\n",
    "        gt_boxes = target[0][\"boxes\"]\n",
    "        gt_labels = target[0][\"labels\"]\n",
    "        \n",
    "        if len(pred_scores) > 0:\n",
    "            avg_confidence = pred_scores.mean().item()\n",
    "            all_conf.append(avg_confidence)\n",
    "        \n",
    "        # Ensure predictions and ground-truth boxes exist before computing IoU\n",
    "        if gt_boxes.numel() == 0 or pred_boxes.numel() == 0:\n",
    "            print(f\" Skipping IoU calculation for image {i} (No objects detected)\")\n",
    "            continue  \n",
    "\n",
    "\n",
    "        iou_matrix = box_iou(gt_boxes, pred_boxes)\n",
    "        max_iou, _ = iou_matrix.max(dim=1)\n",
    "\n",
    "        tp = sum(iou > 0.5 for iou in max_iou)\n",
    "        fp = len(pred_boxes) - tp\n",
    "        fn = len(gt_boxes) - tp\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1_score)\n",
    "\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1s) / len(all_f1s)\n",
    "    avg_confidence = sum(all_conf) / len(all_conf) if all_conf else 0\n",
    "\n",
    "    all_conf.append(avg_confidence)  \n",
    "\n",
    "    return loss_values, avg_precision, avg_recall, avg_f1, all_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2% of dataset...\n",
      "Epoch 1/10, Loss: 0.9370\n",
      "Epoch 2/10, Loss: 0.4160\n",
      "Epoch 3/10, Loss: 0.3301\n",
      "Epoch 4/10, Loss: 0.2715\n",
      "Epoch 5/10, Loss: 0.2952\n",
      "Epoch 6/10, Loss: 0.3159\n",
      "Epoch 7/10, Loss: 0.2592\n",
      "Epoch 8/10, Loss: 0.3125\n",
      "Epoch 9/10, Loss: 0.2568\n",
      "Epoch 10/10, Loss: 0.2636\n",
      " Skipping IoU calculation for image 0 (No objects detected)\n",
      " Skipping IoU calculation for image 1 (No objects detected)\n",
      " Skipping IoU calculation for image 3 (No objects detected)\n",
      " Skipping IoU calculation for image 4 (No objects detected)\n",
      " Skipping IoU calculation for image 5 (No objects detected)\n",
      " Skipping IoU calculation for image 6 (No objects detected)\n",
      " Skipping IoU calculation for image 7 (No objects detected)\n",
      " Skipping IoU calculation for image 8 (No objects detected)\n",
      " Skipping IoU calculation for image 9 (No objects detected)\n",
      " Skipping IoU calculation for image 10 (No objects detected)\n",
      " Skipping IoU calculation for image 11 (No objects detected)\n",
      " Skipping IoU calculation for image 12 (No objects detected)\n",
      " Skipping IoU calculation for image 13 (No objects detected)\n",
      " Skipping IoU calculation for image 14 (No objects detected)\n",
      " Skipping IoU calculation for image 15 (No objects detected)\n",
      " Skipping IoU calculation for image 16 (No objects detected)\n",
      " Skipping IoU calculation for image 17 (No objects detected)\n",
      " Skipping IoU calculation for image 18 (No objects detected)\n",
      " Skipping IoU calculation for image 19 (No objects detected)\n",
      " Skipping IoU calculation for image 20 (No objects detected)\n",
      " Skipping IoU calculation for image 21 (No objects detected)\n",
      " Skipping IoU calculation for image 22 (No objects detected)\n",
      " Skipping IoU calculation for image 23 (No objects detected)\n",
      " Skipping IoU calculation for image 24 (No objects detected)\n",
      " Skipping IoU calculation for image 25 (No objects detected)\n",
      " Skipping IoU calculation for image 26 (No objects detected)\n",
      " Skipping IoU calculation for image 27 (No objects detected)\n",
      " Skipping IoU calculation for image 28 (No objects detected)\n",
      " Skipping IoU calculation for image 29 (No objects detected)\n",
      " Skipping IoU calculation for image 30 (No objects detected)\n",
      " Skipping IoU calculation for image 31 (No objects detected)\n",
      " Skipping IoU calculation for image 33 (No objects detected)\n",
      " Skipping IoU calculation for image 35 (No objects detected)\n",
      " Skipping IoU calculation for image 36 (No objects detected)\n",
      " Skipping IoU calculation for image 37 (No objects detected)\n",
      " Skipping IoU calculation for image 38 (No objects detected)\n",
      " Skipping IoU calculation for image 39 (No objects detected)\n",
      " Skipping IoU calculation for image 40 (No objects detected)\n",
      " Skipping IoU calculation for image 41 (No objects detected)\n",
      " Skipping IoU calculation for image 42 (No objects detected)\n",
      " Skipping IoU calculation for image 43 (No objects detected)\n",
      " Skipping IoU calculation for image 44 (No objects detected)\n",
      " Skipping IoU calculation for image 45 (No objects detected)\n",
      " Skipping IoU calculation for image 47 (No objects detected)\n",
      " Skipping IoU calculation for image 48 (No objects detected)\n",
      "Training with 20% of dataset...\n",
      "Epoch 1/10, Loss: 3.2806\n",
      "Epoch 2/10, Loss: 1.3641\n",
      "Epoch 3/10, Loss: 1.0187\n",
      "Epoch 4/10, Loss: 0.7589\n",
      "Epoch 5/10, Loss: 0.5084\n",
      "Epoch 6/10, Loss: 0.4210\n",
      "Epoch 7/10, Loss: 0.3796\n",
      "Epoch 8/10, Loss: 0.2861\n",
      "Epoch 9/10, Loss: 0.2710\n",
      "Epoch 10/10, Loss: 0.2412\n",
      " Skipping IoU calculation for image 20 (No objects detected)\n",
      " Skipping IoU calculation for image 30 (No objects detected)\n",
      "Training with 40% of dataset...\n",
      "Epoch 1/10, Loss: 1.6594\n",
      "Epoch 2/10, Loss: 0.9075\n",
      "Epoch 3/10, Loss: 0.5810\n",
      "Epoch 4/10, Loss: 0.4815\n"
     ]
    }
   ],
   "source": [
    "# Load Faster R-CNN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 object class + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Run training on different splits\n",
    "precision_vals, recall_vals, f1_vals, loss_vals, conf_vals = [], [], [], [], []\n",
    "train_sizes = [2, 20, 40, 60, 80]  # Training dataset sizes in %\n",
    "\n",
    "for i, (train_subset, test_subset) in enumerate(split_results):\n",
    "    print(f\"Training with {train_sizes[i]}% of dataset...\")\n",
    "\n",
    "    loss_values, precision, recall, f1, conf = train_and_evaluate(model, train_subset, test_subset)\n",
    "    precision_vals.append(precision)\n",
    "    recall_vals.append(recall)\n",
    "    f1_vals.append(f1)\n",
    "    loss_vals.append(loss_values)\n",
    "    conf_vals.append(conf)\n",
    "\n",
    "print(\"Completed: Collected Precision, Recall, F1-score, and Loss values.\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, losses in enumerate(loss_vals):\n",
    "    plt.plot(range(1, 11), losses, label=f\"{train_sizes[i]}% data\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Loss Trend Across Dataset Sizes\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Performance Metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, precision_vals, label=\"Precision\", marker=\"o\", linestyle=\"-\")\n",
    "plt.plot(train_sizes, recall_vals, label=\"Recall\", marker=\"s\", linestyle=\"-\")\n",
    "plt.plot(train_sizes, f1_vals, label=\"F1 Score\", marker=\"d\", linestyle=\"-\")\n",
    "\n",
    "plt.xlabel(\"Training Data Percentage\")\n",
    "plt.ylabel(\"Performance Metrics\")\n",
    "plt.title(\"Model Performance with Increasing Training Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision vs. Recall Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recall_vals, precision_vals, label=\"Precision vs Recall\", marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot F1-score vs Confidence Level\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(conf_vals, f1_vals, label=\"F1-score vs Confidence\", marker=\"s\", linestyle=\"-\")\n",
    "plt.xlabel(\"Confidence Score\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1-score vs Confidence Level\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Training images: 40, Testing images: 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "train_img_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/images\"\n",
    "train_ann_file = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/train/dataset_coco.json\"\n",
    "test_img_dir = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/images\"\n",
    "test_ann_file = \"/Users/bhavnasharma/Downloads/SinGAN/Faster-R-CNN/test/dataset_coco.json\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file, transform=ToTensor())\n",
    "test_dataset = CocoDetection(root=test_img_dir, annFile=test_ann_file, transform=ToTensor())\n",
    "\n",
    "# DataLoaders with improved collation to prevent empty target mismatches\n",
    "# Function to correctly collate images and annotations\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # Separate images and annotations\n",
    "    formatted_images, formatted_targets = [], []\n",
    "\n",
    "    # Create a mapping from image_id to its annotations\n",
    "    image_annotations = defaultdict(list)\n",
    "    for anns in targets:\n",
    "        for ann in anns:\n",
    "            image_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # Assign correct annotations to each image\n",
    "    for img, anns in zip(images, targets):\n",
    "        if anns:  # Ensure annotations exist\n",
    "            image_id = anns[0][\"image_id\"]  # Get the image ID\n",
    "            matched_annotations = image_annotations[image_id]  # Retrieve corresponding bboxes\n",
    "\n",
    "            valid_boxes = torch.tensor([ann[\"bbox\"] for ann in matched_annotations], dtype=torch.float32)\n",
    "            valid_labels = torch.tensor([ann[\"category_id\"] for ann in matched_annotations], dtype=torch.int64)\n",
    "\n",
    "            formatted_target = {\n",
    "                \"boxes\": valid_boxes,\n",
    "                \"labels\": valid_labels\n",
    "            }\n",
    "\n",
    "            formatted_images.append(img)\n",
    "            formatted_targets.append(formatted_target)\n",
    "\n",
    "    return formatted_images, formatted_targets  # Ensure proper batching\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training images: {len(train_dataset)}, Testing images: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify model for custom dataset\n",
    "num_classes = 2  # 1 object class + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-2b10c1748f9b>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_boxes = torch.tensor(t[\"boxes\"], dtype=torch.float32).to(device)\n",
      "<ipython-input-20-2b10c1748f9b>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_labels = torch.tensor(t[\"labels\"], dtype=torch.int64).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.3301\n",
      "Epoch 2/10, Loss: 1.9541\n",
      "Epoch 3/10, Loss: 1.3014\n",
      "Epoch 4/10, Loss: 0.8848\n",
      "Epoch 5/10, Loss: 0.6640\n",
      "Epoch 6/10, Loss: 0.5398\n",
      "Epoch 7/10, Loss: 0.4314\n",
      "Epoch 8/10, Loss: 0.3545\n",
      "Epoch 9/10, Loss: 0.3187\n",
      "Epoch 10/10, Loss: 0.2909\n",
      "Training complete! \n"
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "\n",
    "        if len(targets) == 0:\n",
    "            print(\"Skipping batch with no annotations\")\n",
    "            continue  # Skip empty batches\n",
    "\n",
    "        # Convert targets into correct format for Faster R-CNN\n",
    "        formatted_targets = []\n",
    "        for t in targets:\n",
    "            valid_boxes = torch.tensor(t[\"boxes\"], dtype=torch.float32).to(device)\n",
    "            valid_labels = torch.tensor(t[\"labels\"], dtype=torch.int64).to(device)\n",
    "\n",
    "            formatted_target = {\n",
    "                \"boxes\": valid_boxes,\n",
    "                \"labels\": valid_labels\n",
    "            }\n",
    "            formatted_targets.append(formatted_target)\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, formatted_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[212.6916,   0.0000, 241.0656,  46.1231],\n",
      "        [133.8734,  39.6070, 149.8190,  64.0690],\n",
      "        [ 71.9773,   1.1935,  97.3838,  42.5187],\n",
      "        [ 18.7041,   7.3625,  58.8743,  73.7856],\n",
      "        [ 18.8431,   7.2597,  32.5765,  31.5699]]), 'labels': tensor([1, 1, 1, 1, 1]), 'scores': tensor([0.9945, 0.9936, 0.9929, 0.9820, 0.2514])}]\n",
      "[{'boxes': tensor([[202.6065,  20.8546, 231.1020,  68.3693],\n",
      "        [139.3871,   1.0539, 180.0548,  67.5907],\n",
      "        [161.3741,   8.4370, 178.4475,  68.2247]]), 'labels': tensor([1, 1, 1]), 'scores': tensor([0.9956, 0.9877, 0.2968])}]\n",
      "[{'boxes': tensor([[188.6717,   2.9398, 228.2422,  71.8211],\n",
      "        [152.0560,  13.2947, 164.1678,  33.6640]]), 'labels': tensor([1, 1]), 'scores': tensor([0.9930, 0.9876])}]\n",
      "[{'boxes': tensor([[100.8294,   1.2967, 125.8417,  45.4742],\n",
      "        [201.0968,  10.4852, 231.1635,  59.2066],\n",
      "        [ 14.3974,  31.2924,  33.8708,  63.8870],\n",
      "        [149.4897,   4.3518, 170.9699,  39.7663],\n",
      "        [ 61.5472,   5.2919,  78.8884,  34.1479]]), 'labels': tensor([1, 1, 1, 1, 1]), 'scores': tensor([0.9963, 0.9951, 0.9951, 0.9948, 0.9909])}]\n",
      "[{'boxes': tensor([[  4.7029,   1.3007,  41.2778,  63.4448],\n",
      "        [ 71.3242,   0.0000, 105.9854,  59.0746]]), 'labels': tensor([1, 1]), 'scores': tensor([0.9926, 0.9910])}]\n",
      "[{'boxes': tensor([[ 29.8478,   9.1466,  66.4921,  72.9313],\n",
      "        [ 96.2479,  24.6232, 114.8245,  54.8032],\n",
      "        [204.4998,   6.2150, 239.5486,  66.0543],\n",
      "        [157.2206,   5.9228, 188.4650,  57.7961],\n",
      "        [176.0299,   0.0000, 240.0838,  66.7339]]), 'labels': tensor([1, 1, 1, 1, 1]), 'scores': tensor([0.9943, 0.9912, 0.9909, 0.9885, 0.3229])}]\n",
      "[{'boxes': tensor([[ 48.7247,   1.6687,  71.6728,  41.0308],\n",
      "        [126.5083,  13.5971, 157.4329,  65.4665],\n",
      "        [210.7522,   9.9450, 233.0013,  47.9400]]), 'labels': tensor([1, 1, 1]), 'scores': tensor([0.9945, 0.9929, 0.9920])}]\n",
      "[{'boxes': tensor([[ 76.6888,  12.9165, 109.9935,  68.3124]]), 'labels': tensor([1]), 'scores': tensor([0.9929])}]\n",
      "[{'boxes': tensor([[155.8701,  17.1036, 177.7819,  52.7371],\n",
      "        [210.5854,   0.0000, 246.9451,  62.8600]]), 'labels': tensor([1, 1]), 'scores': tensor([0.9946, 0.9919])}]\n",
      "[{'boxes': tensor([[ 10.5320,   6.2092,  51.0053,  73.1747],\n",
      "        [221.7876,   1.9557, 246.6837,  44.4166],\n",
      "        [160.3919,   6.4416, 197.9489,  70.9851]]), 'labels': tensor([1, 1, 1]), 'scores': tensor([0.9939, 0.9938, 0.9937])}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "for i, (image, _) in enumerate(test_loader):\n",
    "    image_tensor = image[0].to(device)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        output = model([image_tensor])  # Run inference without targets\n",
    "\n",
    "    print(output)  # Check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.93, Average Recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import box_iou  \n",
    "\n",
    "model.eval()  \n",
    "\n",
    "all_precisions, all_recalls = [], []\n",
    "\n",
    "for i, (image, target) in enumerate(test_loader):\n",
    "    image_tensor = image[0].to(device)\n",
    "\n",
    "    with torch.no_grad():  # No gradients for inference\n",
    "        output = model([image_tensor])\n",
    "\n",
    "    pred_boxes = output[0][\"boxes\"].cpu()\n",
    "    pred_labels = output[0][\"labels\"].cpu()\n",
    "    pred_scores = output[0][\"scores\"].cpu()\n",
    "\n",
    "    gt_boxes = target[0][\"boxes\"]\n",
    "    gt_labels = target[0][\"labels\"]\n",
    "\n",
    "    iou_matrix = box_iou(gt_boxes, pred_boxes)\n",
    "    max_iou, _ = iou_matrix.max(dim=1)  \n",
    "\n",
    "    iou_threshold = 0.5\n",
    "    tp = sum(iou > iou_threshold for iou in max_iou)\n",
    "    fp = len(pred_boxes) - tp\n",
    "    fn = len(gt_boxes) - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "\n",
    "\n",
    "# Compute Average Precision & Recall across all test images\n",
    "avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.2f}, Average Recall: {avg_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
